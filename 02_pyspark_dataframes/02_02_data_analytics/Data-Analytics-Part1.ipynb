{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames: Data Analysis Part 1\n",
    "\n",
    "Welcome to the second part of the PySpark DataFrames module! In this part, we will focus on more advanced operations on DataFrames to perform data analysis and manipulation.\n",
    "\n",
    "There are plenty of operations that can be performed on DataFrames. Therefore, this notebook will cover the most common and useful ones by answering some questions about the data we've preprocessed in the first part.\n",
    "\n",
    "This notebook is composed by a set of questions that will teach you how to use some pyspark DataFrame methods and SQL functions that are very useful for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let's get the orders and products data we've saved in the DBFS in the first part of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"/FileStore/lp-big-data/orders-data/orders-preprocessed.csv\")\n",
    ")\n",
    "\n",
    "df_products = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"/FileStore/lp-big-data/orders-data/products-preprocessed.csv\")\n",
    ")\n",
    "\n",
    "df_orders.display()\n",
    "df_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join tables\n",
    "\n",
    "Our two tables complement each other. The orders table provides information about the orders, each comprising one or more units of a product.\n",
    "\n",
    "The common column between them is `product_id`. We can merge them based on this column to extract relevant statistics.\n",
    "\n",
    "But how do we merge two tables?\n",
    "\n",
    "Spark DataFrames offer similar join operations to those found in SQL.\n",
    "\n",
    "To use the `.join` operator, we need to specify:\n",
    "- Two tables, referred to as the left and right tables, respectively.\n",
    "- One or more predicates, which are the conditions determining how records from the two tables are matched.\n",
    "- Finally, a method indicating how the join is performed when the predicate succeeds and when it fails.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "```\n",
    "left_table.join(\n",
    "  right_table,\n",
    "  on=[predicates],\n",
    "  how='join_method'\n",
    ")\n",
    "```\n",
    "\n",
    "The predicates in a PySpark join are rules applied between columns of the left and right data frames. The join occurs record-wise, comparing each record in the left data frame to each record in the right data frame based on the predicates. If the predicates return `True`, it's a match; otherwise, it's a no-match.\n",
    "\n",
    "\n",
    "Similarly to SQL, there are several join methods available in PySpark, with the default one being the `inner` join.\n",
    "\n",
    "The following image summarizes the most common join strategies. For a detailed tutorial on PySpark joins check this [link](https://www.geeksforgeeks.org/pyspark-join-types-join-two-dataframes/).\n",
    "\n",
    "![Join types](https://github.com/inesmcm26/lp-big-data-mercedes/blob/main/data/images/joins-in-mysql.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the join operator to join the orders and products tables.\n",
    "\n",
    "We want to use a left join on the orders table because we don't want to miss any orders data, even the products sold on that order are not listed on the products table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=['product_id'],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_orders_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We joined the two tables using a column that is present in both tables. Because of that, we ended up with a table that has only one column `product_id`, resulting from collapsing these columns from both tables.\n",
    "\n",
    "But there could be the case that the column we're joining on has not the exact same name in both tables. In that case we would have to specify the name of the column of each table.\n",
    "\n",
    "We can still do it that way, even if the column we're joining on has the same name in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products_duplicate = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=df_orders['product_id']==df_products['product_id'],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Or\n",
    "\n",
    "df_orders_products_duplicate = (\n",
    "    df_orders.alias('orders')\n",
    "    .join(\n",
    "        df_products.alias('products'),\n",
    "        on=[f.col('orders.product_id') == f.col('products.product_id')],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_orders_products_duplicate.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we end up with two `product_id` columns. In this case, we can simply drop one of them. We need to specify which of the columns we want to drop, otherwise, both columns will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products = (\n",
    "    df_orders_products_duplicate\n",
    "    .drop(df_products['product_id'])\n",
    ")\n",
    "\n",
    "df_orders_products.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tables merged, let's answer some questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many unique customers placed orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products.select(\"customer_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing it is by using the `countDistinct()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products.select(f.countDistinct('order_id')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the total profit generated from all orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do it using an aggregation function\n",
    "(\n",
    "    df_orders_products\n",
    "    .select(\n",
    "        f.sum(f.col('profit')).alias('total_profit')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or using RDD functions\n",
    "profit_list = (\n",
    "    df_orders_products\n",
    "    .select(f.col('profit'))\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "sum(profit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When was the first time each customer placed an order?\n",
    "\n",
    "To answer this question with need to group information by customer and then aggregate the data to get the first order date.\n",
    "\n",
    "But first, we need to sort the orders by the placing date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  df_orders_products\n",
    "  .orderBy('placing_date')\n",
    "  .groupBy('customer_id')\n",
    "  .agg(\n",
    "      f.first('placing_date').alias('first_placing_date')\n",
    ")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the name of the slowest product to be delivered on average?\n",
    "\n",
    "Remember that we created a column `days_to_delivery`? Let's use it to answer this question.\n",
    "\n",
    "Once again, we want to group information by product and then apply an aggregation function that gives us the average delivery time.\n",
    "\n",
    "After that, we can sort the results in descending order to get the slowest product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy(['product_id', 'product_name'])\n",
    "    .agg(\n",
    "        f.avg('days_to_delivery').alias('avg_delivery_delay')\n",
    "    )\n",
    "    .sort(f.desc('avg_delivery_delay'))\n",
    "    .select('product_name')\n",
    "    .limit(1)\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many orders with category `Shoes` were placed during the weekends (week days 6 and 7)? \n",
    "\n",
    "To answer this question we do not want to aggregate data. We want to filter the data based on the category and the day of the week and then count the number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .filter(f.col('product_category') == 'Shoes')\n",
    "    .filter(f.col('order_day_of_week').isin([6, 7]))\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the ID of the most ordered product for each delivery speed?\n",
    "\n",
    "Now we need to group the data based on not only one, but two columns.\n",
    "\n",
    "First, we need to get the number of times each product is ordered for each delivery speed.\n",
    "\n",
    "Then, we can get the product with the highest number of orders for each delivery speed and the corresponding number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of each product for each delivery speed\n",
    "product_count_df = (\n",
    "    df_orders_products\n",
    "    .groupBy(['delivery_speed', 'product_id'])\n",
    "    .agg(f.count('order_id').alias('nr_orders'))\n",
    ")\n",
    "\n",
    "# Find the product with the maximum number of orders for each delivery speed\n",
    "result_df = (\n",
    "    product_count_df\n",
    "    .orderBy(['delivery_speed', f.desc('nr_orders')])\n",
    "    .groupBy('delivery_speed')\n",
    "    .agg(\n",
    "        f.first('product_id').alias('most_ordered_product_id'),\n",
    "        f.max('nr_orders').alias('max_orders')\n",
    "    )\n",
    ")\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which product category has the highest total sales revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('product_category')\n",
    "    .agg(f.sum('revenue').alias('total_revenue'))\n",
    "    .sort(f.desc('total_revenue'))\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For each customer segment, what is the maximum delivery delay and the average profit?\n",
    "\n",
    "We can perform multiple aggregations on the same group of data at once as we've seen in question 6. Let's see another syntax to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('customer_status')\n",
    "    .agg({\n",
    "        'days_to_delivery' : 'max',\n",
    "        'profit' : 'avg',\n",
    "    })\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the average number of units (amount) per order of each product sold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or using\n",
    "(\n",
    "    df_orders_products\n",
    "    .groupBy(f.col('product_id'))\n",
    "    .agg(f.avg('amount').alias('avg_amount'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What are the dates in which each product was delivered among all orders?\n",
    "\n",
    "To answer this question we need to group the data by product and then aggregate the dates in which the product was delivered on a list. We can use the `collect_list()` function to do that.\n",
    "\n",
    "This aggregation function is different from the ones we've seen so far. Instead of collapsing all the data into a single value, it creates a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy(f.col('product_id'))\n",
    "    .agg(f.collect_list('delivery_date').alias('delivery_dates'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How many orders of each category were placed in each year?\n",
    "\n",
    "To answer this question we can use the pivot operation. This operation produces a pivot table which is a cross-tabulation that can show the relationship between two columns.\n",
    "\n",
    "As an alternative we could also use the groupBy operation, but the pivot operation results in a more readable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupBy\n",
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('order_year', 'product_category')\n",
    "    .agg(f.count('order_id'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pivot\n",
    "pivot_df = (\n",
    "    df_orders_products\n",
    "    .groupBy('order_year')\n",
    "    .pivot('product_category')\n",
    "    .agg(f.count('order_id'))\n",
    ")\n",
    "\n",
    "pivot_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query created one row each element in the `groupBy` clause and one column for each unique value in the `pivot` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What is the total profit each continent suppliers generated each year?\n",
    "\n",
    "Again, we can use the pivot operation to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = (\n",
    "    df_orders_products\n",
    "    .groupBy('order_year')\n",
    "    .pivot('supplier_continent')\n",
    "    .agg(f.sum('profit'))\n",
    ")\n",
    "\n",
    "pivot_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Who is the client that bought the greatest variety of products in each year?\n",
    "\n",
    "Let's break the question down into smaller parts:\n",
    "- First, we need to count the number of unique products each customer bought in each year.\n",
    "- Then, we need to get the customer with the highest number of unique products bought in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique products each client ordered in each year\n",
    "unique_product_count_df = (\n",
    "    df_orders_products\n",
    "    .groupBy(['customer_id', 'order_year'])\n",
    "    .agg(f.countDistinct('product_id').alias('nr_unique_products'))\n",
    ")\n",
    "\n",
    "# Find the customer with the maximum number of unique products for each year\n",
    "result_df = (\n",
    "    unique_product_count_df\n",
    "    .orderBy(['order_year', f.desc('nr_unique_products')])\n",
    "    .groupBy('order_year')\n",
    "    .agg(f.first('customer_id').alias('most_varied_customer'), f.max('nr_unique_products').alias('max_unique_products'))\n",
    ")\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. What is the standard deviation of profit for each supplier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('supplier_id')\n",
    "    .agg(f.stddev('profit').alias('profit_stddev'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. How many unique customers placed orders within each supplier continent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('supplier_continent')\n",
    "    .agg(\n",
    "        f.countDistinct('customer_id')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Great! We've gone through a lot of questions and have used several of the most used PySpark DataFrame methods and SQL functions.\n",
    "\n",
    "Now it's time to solve some exercises to practice what we've learned.\n",
    "\n",
    "Go to notebook `exercises-part2` to solve the exercises."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
