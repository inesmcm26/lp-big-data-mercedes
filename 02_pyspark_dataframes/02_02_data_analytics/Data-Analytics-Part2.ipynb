{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames: Data Analysis Part 2\n",
    "\n",
    "Welcome to the last notebooks of the PySpark DataFrames module! This part is a continuation of the previous one, where we will continue practicing more advanced operations on PySpark DataFrames.\n",
    "\n",
    "This notebook is composed by more questions that will teach you how to use some pyspark DataFrame methods and SQL functions that are very useful for data analysis.\n",
    "\n",
    "The data used in this notebook is the same as the previous one, the orders and products datasets adapted from the JMP Case Study Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the orders and products data we've saved in the DBFS in the first part of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"/FileStore/lp-big-data/orders-data/orders-preprocessed.csv\")\n",
    ")\n",
    "\n",
    "df_products = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"/FileStore/lp-big-data/orders-data/products-preprocessed.csv\")\n",
    ")\n",
    "\n",
    "df_orders.display()\n",
    "df_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join tables\n",
    "\n",
    "Once again, let's use the join operator to join the orders and products tables.\n",
    "\n",
    "We want to use a left join on the orders table because we don't want to miss any orders data, even the products sold on that order are not listed on the products table.\n",
    "\n",
    "As we've seen in the previous notebook, since both tables have a column with the same name, we can simply use that column to join the tables instead of having to seperately specify the name of the column in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=['product_id'],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_orders_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the schema of the merged table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics - Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each customer segment, what is the maximum delivery delay and the average profit?\n",
    "\n",
    "We can perform multiple aggregations on the same group of data at once as we've seen in question 6 of the previous notebook. Let's see another syntax to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('customer_status')\n",
    "    .agg({\n",
    "        'days_to_delivery' : 'max',\n",
    "        'profit' : 'avg',\n",
    "    })\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the average number of units (amount) per order of each product sold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or using\n",
    "(\n",
    "    df_orders_products\n",
    "    .groupBy(f.col('product_id'))\n",
    "    .agg(f.avg('amount').alias('avg_amount'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the dates in which each product was delivered among all orders?\n",
    "\n",
    "To answer this question we need to group the data by product and then aggregate the dates in which the product was delivered on a list. We can use the `collect_list()` function to do that.\n",
    "\n",
    "This aggregation function is different from the ones we've seen so far. Instead of collapsing all the data into a single value, it creates a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy(f.col('product_id'))\n",
    "    .agg(f.collect_list('delivery_date').alias('delivery_dates'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many orders of each category were placed in each year?\n",
    "\n",
    "To answer this question we can use the pivot operation. This operation produces a pivot table which is a cross-tabulation that can show the relationship between two columns.\n",
    "\n",
    "As an alternative we could also use the groupBy operation, but the pivot operation results in a more readable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupBy\n",
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('order_year', 'product_category')\n",
    "    .agg(f.count('order_id'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pivot\n",
    "pivot_df = (\n",
    "    df_orders_products\n",
    "    .groupBy('order_year')\n",
    "    .pivot('product_category')\n",
    "    .agg(f.count('order_id'))\n",
    ")\n",
    "\n",
    "pivot_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query created one row each element in the `groupBy` clause and one column for each unique value in the `pivot` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the total profit each continent suppliers generated each year?\n",
    "\n",
    "Again, we can use the pivot operation to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = (\n",
    "    df_orders_products\n",
    "    .groupBy('order_year')\n",
    "    .pivot('supplier_continent')\n",
    "    .agg(f.sum('profit'))\n",
    ")\n",
    "\n",
    "pivot_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Who is the client that bought the greatest variety of products in each year?\n",
    "\n",
    "Let's break the question down into smaller parts:\n",
    "- First, we need to count the number of unique products each customer bought in each year.\n",
    "- Then, we need to get the customer with the highest number of unique products bought in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique products each client ordered in each year\n",
    "unique_product_count_df = (\n",
    "    df_orders_products\n",
    "    .groupBy(['customer_id', 'order_year'])\n",
    "    .agg(f.countDistinct('product_id').alias('nr_unique_products'))\n",
    ")\n",
    "\n",
    "# Find the customer with the maximum number of unique products for each year\n",
    "result_df = (\n",
    "    unique_product_count_df\n",
    "    .orderBy(['order_year', f.desc('nr_unique_products')])\n",
    "    .groupBy('order_year')\n",
    "    .agg(f.first('customer_id').alias('most_varied_customer'), f.max('nr_unique_products').alias('max_unique_products'))\n",
    ")\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the standard deviation of profit for each supplier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('supplier_id')\n",
    "    .agg(f.stddev('profit').alias('profit_stddev'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How many unique customers placed orders within each supplier continent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders_products\n",
    "    .groupBy('supplier_continent')\n",
    "    .agg(\n",
    "        f.countDistinct('customer_id')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Great work! We've learned a lot about PySpark DataFrames and SQL functions and how to use them to perform data analysis.\n",
    "\n",
    "As usual, it's time to practice what we've learned. Go to notebook `exercises-part2` to solve the exercises."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
