# PySpark DataFrames Basics

Welcome to the PySpark DataFrames Basics module! ✴️

This module introduces the basics of working with DataFrames in PySpark. DataFrames are a key abstraction in PySpark that provide a more user-friendly interface for working with structured data.

In this module, you will learn how to create DataFrames, perform basic operations on them, and understand the underlying concepts that drive PySpark's DataFrame API.

To do so, you'll be looking into orders data from an e-commerce platform. You'll be using PySpark to load this data into a DataFrame and perform various operations on it to answer business questions.

## Submodules

### 1. Data Preprocessing

    Introduction to PySpark DataFrames, covering their key features, advantages over RDDs, how to create DataFrames from different data sources and basic operations like selecting, filtering and creating new columns.

    Also explores a lot of PySpark SQL functions that can be used to manipulate DataFrames.

    This submodule contains one theoretical and one practical notebook and is expected to take about 2 hours to complete.

### 2. Data Analytics

    Explores more advanced operations on PySpark DataFrames, including grouping and aggregation, sorting and joining.

    This submodule contains two theoretical and two practical notebooks and is expected to take about 4 hours to complete (2h hours per notebook + exercises)


## Running the Notebooks

All notebooks in this module are designed to be run in the **Databricks Community Edition**. Detailed steps to set up and configure your environment are provided in the first module.

If you need, go back to `2-Databricks-Environment` notebook in module `01_spark_intro` and follow the instructions there to ensure you have the necessary setup to run these notebooks successfully.

---

Happy Learning!