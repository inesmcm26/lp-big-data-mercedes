{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've learned about visualizing PySpark DataFrame properties, data cleaning using PySpark, and how to create new features.\n",
    "\n",
    "Now it's time to apply this knowledge to a new dataset.\n",
    "\n",
    "In this notebook, you'll work with the `product-supplier` dataset that we've downloaded along with the `orders` dataset. \n",
    "\n",
    "The file is saved in the dbfs at `/FileStore/lp-big-data/orders-data/product-supplier.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset into a spark dataframe and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = spark.read.csv(\n",
    "    \"/FileStore/lp-big-data/orders-data/product-supplier.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    ")\n",
    "\n",
    "# OR\n",
    "\n",
    "df_products = (\n",
    "    spark.read.format('csv')\n",
    "    .option('inferSchema', 'true')\n",
    "    .option('header', 'true')\n",
    "    .option('sep', ',')\n",
    "    .load('/FileStore/lp-big-data/orders-data/product-supplier.csv')\n",
    ")\n",
    "\n",
    "df_products.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many observations does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Rename the columns to standardize them.\n",
    "\n",
    "- Lower case all column names\n",
    "- Replace the spaces with underscores `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using the toDF() method specifying all the column names\n",
    "df_products_renamed = (\n",
    "    df_products\n",
    "    .toDF('product_id',\n",
    "        'product_line',\n",
    "        'product_category',\n",
    "        'product_group',\n",
    "        'product_name',\n",
    "        'supplier_country',\n",
    "        'supplier_name',\n",
    "        'supplier_id',\n",
    "        'announcement_date',\n",
    "        'launch_date',\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_renamed.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using a for loop and pure python to create\n",
    "# new column names and using the toDF() method to rename the columns\n",
    "\n",
    "# Get a list of strings with the column names\n",
    "column_names_list = df_products.columns\n",
    "\n",
    "# Create an empty list to save the new column names\n",
    "column_names_new = []\n",
    "\n",
    "# Loop through the column names list\n",
    "for col_name in column_names_list:\n",
    "    # Lowercase the column name and replace spaces with underscores\n",
    "    column_names_new.append(col_name.lower().replace(' ', '_'))\n",
    "\n",
    "# Rename the columns\n",
    "df_products_renamed = df_products.toDF(*column_names_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check if there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_renamed.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the missing categories by 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_filled = (\n",
    "    df_products_renamed\n",
    "    .fillna('Unknown', subset=(['product_category']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the missing values have been filled\n",
    "df_products_filled.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the names of all possible product lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Get them visually in a table, along with the count of each product line\n",
    "df_products_filled.groupBy('product_line').count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using the rdd.flatMap().collect() method to get the product lines in a list\n",
    "df_products_filled.select('product_line').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Selecting the distinct values, transform the resulting\n",
    "# pySpark DataFrame into a pandas DataFrame and use the .values attribute\n",
    "df_products_filled.select('product_line').distinct().toPandas().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create a new column called `lead_time` with the amount of time in days between each product's announcement and launch dates\n",
    "\n",
    "Hint: Check out the [DateTime functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#datetime-functions) from the PySpark SQL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_lead = (\n",
    "    df_products_filled\n",
    "    .withColumn(\n",
    "        'lead_time',\n",
    "        f.datediff(f.col('launch_date'), f.col('announcement_date'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_lead.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The announcement and launch dates are in UTC format. Convert them to New York's timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_timezone = (\n",
    "    df_products_lead\n",
    "    .withColumn('announcement_date', f.from_utc_timestamp(f.col('announcement_date'), 'America/New_York'))\n",
    "    .withColumn('launch_date', f.from_utc_timestamp(f.col('launch_date'), 'America/New_York'))\n",
    ")\n",
    "\n",
    "df_products_timezone.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a new column with the suppliers' continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the possible values for the supplier_country column and their counts\n",
    "df_products_timezone.groupBy('supplier_country').count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_preprocessed = (\n",
    "    df_products_timezone\n",
    "    # Define to which continent the supplier belongs based on the country\n",
    "    .withColumn(\n",
    "        'supplier_continent',\n",
    "        f.when(\n",
    "            f.col('supplier_country').isin(\n",
    "                ['NL', 'PT', 'GB', 'DE', 'ES', 'FR', 'NO', 'DK', 'BE', 'SE']\n",
    "                ),\n",
    "            'Europe'\n",
    "        )\n",
    "        .when(f.col('supplier_country').isin(['CA', 'US']), 'America')\n",
    "        .when(f.col('supplier_country').isin(['AU']), 'Oceania')\n",
    "        .otherwise('Unkown')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_products_preprocessed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Save the data to the dbfs to the folder `/FileStore/lp-big-data/orders-data/` under the name `products-preprocessed.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_preprocessed.write.csv('/FileStore/lp-big-data/orders-data/products-preprocessed.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
