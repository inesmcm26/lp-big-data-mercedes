{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames - Part 1\n",
    "\n",
    "In this notebook, we will learn how to work with PySpark DataFrames. We will see how to create DataFrames, perform operations on them, and save them.\n",
    "\n",
    "**A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database**. It is part of the PySpark library, and can be created using various functions in SparkSession.\n",
    "\n",
    "DataFrames in PySpark are conceptually equivalent to pandas DataFrames, but they are distributed across a cluster and designed to handle large-scale data processing.\n",
    "\n",
    "To understand PySpark DataFrames, it is important to first understand RDDs, the building blocks of PySpark that we explored in the previous module. **PySpark DataFrames are built on top of RDD**s, but they provide a more user-friendly API for manipulating data. Here are three key functionalities of PySpark DataFrames that distinguish them from RDDs:\n",
    "\n",
    "- **Schema information**: Unlike RDDs, DataFrames have schema information, meaning they have named columns and defined data types.\n",
    "- **Ease of use**: DataFrames provide high-level methods and operations similar to those found in SQL and pandas, making them more user-friendly.\n",
    "- **Flexibility**: DataFrames are more flexible and expressive than RDDs. They allow you to perform complex operations with less code.\n",
    "\n",
    "For a more detailed comparison between RDDs and DataFrames, please go back to the notebook `3-PySpark-RDDs` of the previous module.\n",
    "\n",
    "Once created, DataFrames can be manipulated using the various methods that we'll explore in this notebook.\n",
    "\n",
    "***Note:*** As we've seen in the previous notebooks, databricks cluster initialization automatically creates a SparkSession object called `spark` which we can use to create DataFrames.\n",
    "\n",
    "Let's start by exploring the different data sources that can be used to create a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs\n",
    "\n",
    "You can create a DataFrame from an existing RDD. Here's an example of how to create a DataFrame from an RDD of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD using spark context\n",
    "rdd = sc.parallelize([(1, \"John\"), (2, \"Isabel\"), (3, \"Bart\")])\n",
    "\n",
    "# Create DataFrame from RDD\n",
    "df = spark.createDataFrame(rdd, [\"ID\", \"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames also have the lazy evaluation property as they are built on top of RDDs. This means that transformations on DataFrames are not computed until an action is executed.\n",
    "\n",
    "In the previous cell we have created a DataFrame, but no action was executed. On the next cell we'll call an action to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Tuples\n",
    "\n",
    "You can create a DataFrame from a list of tuples, where each tuple represents a row. Then, you can specify the column names separately when creating the DataFrame.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"John\"), (2, \"Isabel\"), (3, \"Bart\")]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\"])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Dictionaries\n",
    "\n",
    "Similarly to Pandas, you can create a DataFrame from a list of dictionaries. Each dictionary represents a row that maps the columns names to the values.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"ID\": 1, \"Name\": \"John\"}, {\"ID\": 2, \"Name\": \"Isabel\"}, {\"ID\": 3, \"Name\": \"Bart\"}]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame\n",
    "\n",
    "You can also create a PySpark DataFrame directly from a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\"ID\": [1, 2, 3], \"Name\": [\"John\", \"Isabel\", \"Bart\"]})\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "spark_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can convert a PySpark DataFrame to a Pandas DataFrame using the `toPandas()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_df.toPandas()\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "You can create a DataFrame from a CSV file.\n",
    "\n",
    "Let's download a CSV file and create a DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "wget https://raw.githubusercontent.com/inesmcm26/lp-big-data/main/data/people-100.csv -O people.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid saving this data to the DBFS, let's read it directly from the driver's local file system. Remember you need to specify the full path to the file, beginning with `file:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do it like this\n",
    "df = spark.read.csv(\"file:/databricks/driver/people.csv\", header=True, sep=',', inferSchema=True)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or like this to make it more readable\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"file:/databricks/driver/people.csv\")\n",
    ")\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the schema of the data you're reading.\n",
    "\n",
    "The schema of a DataFrame defines the column names, each column data type and whether `null` values are accepted in that column.\n",
    "\n",
    "In the last cell, we used the option `.option(\"inferSchema\", \"true\")` when reading the data. By doing this, we are asking spark to find out the structure of the data before creating our DataFrame.\n",
    "\n",
    "However, instead of relying on Spark to infer the schema, you can explicitly define the structure of the table.\n",
    "\n",
    "This is extremely useful when you want to improve the robustness of your data pipeline; **it is better to know you are missing a few columns at ingestion time than to get an error later in the program**. Additionally, defining the schema in advance can improve performance because inferring the schema requires a pre-read of the data.\n",
    "\n",
    "In PySpark, we have a specific data type to represent schemas: the `StructType`.\n",
    "\n",
    "You can think of a Struct as a dictionary, and its pairs of key and values are `StructFields`, in which you specify the column name, the data type and if the column can have null values.\n",
    "\n",
    "All the data types available for each column are described in the [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, DateType, StructType, StructField\n",
    "\n",
    "people_df_schema = StructType([\n",
    "    StructField('Index', IntegerType(), False),\n",
    "    StructField('User Id', StringType(), True),\n",
    "    StructField('First Name', StringType(), True),\n",
    "    StructField('Last Name', StringType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Email', StringType(), True),\n",
    "    StructField('Phone', StringType(), True),\n",
    "    StructField('Date of birth', DateType(), True),\n",
    "    StructField('Job Title', StringType(), True),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    spark.read.format('csv')\n",
    "    .schema(people_df_schema)\n",
    "    .option('header', 'true')\n",
    "    .option('sep', ',')\n",
    "    .load('file:/databricks/driver/people.csv')\n",
    ")\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Databases\n",
    "\n",
    "You can also create a DataFrame from an external database like MySQL, PostgreSQL, etc. You need to specify the JDBC URL, the table name, and the connection properties.\n",
    "\n",
    "The code below is just an example. It won't run because we don't have an external database running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://hostname:port/database_name\") \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "You can also create/save a DataFrame from/to a table. \n",
    "\n",
    "Let's first save a simple DataFrame to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame({\"ID\": ['1', '2', '3'], \"Name\": [\"John\", \"Isabel\", \"Bart\"]})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.write.saveAsTable('users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Databricks, when you use the `saveAsTable('name')` method on a PySpark DataFrame, **the data is stored in a managed table within the Databricks environment**. Databricks manages the underlying storage for you, so you don't need to worry about the specifics of the distributed file system.\n",
    "\n",
    "**The data is typically stored in a distributed storage layer managed by Databricks, which abstracts away the details of the storage infrastructure.** Databricks manages this storage layer transparently, allowing you to focus on your data analysis tasks without needing to worry about infrastructure management.\n",
    "\n",
    "**To access the data stored in the table, you can use SQL in Databricks or the PySpark DataFrame API**. Databricks provides a unified interface for accessing and querying data, regardless of its underlying storage. So, you can interact with the table just like any other table in your Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the table was created by running a SQL query directly on the Databricks environment. As we've seen in the previous module, you can do this by using the `%sql` magic command\n",
    "\n",
    "When running this command, **you can access any data that is referenced in the metastore of the running cluster**. Since we've saved the DataFrame as a table, a reference to the table is stored in the metastore and can be accessed using the `%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is stored in a table, you can read it into a spark DataFrame using the `spark.read.table()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table('users').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrame Operations - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark DataFrame Operations VS SQL Queries\n",
    "\n",
    "PySpark DataFrame methods provide similar functionality as SQL queries when it comes to data manipulation and transformation tasks. \n",
    "\n",
    "In fact, you can basically **run all DataFrame methods as SQL queries by using the `sql` method of the SparkSession, or the `%sql` magic on a Databricks cell**. **Both methods are optimized to run efficiently on Spark clusters**.\n",
    "\n",
    "However, **DataFrame methods are more flexible** and can be used in more complex scenarios than SQL queries. They are also more readable and **easier to maintain**. Here are some key advantages of using DataFrame methods over SQL queries:\n",
    "\n",
    "- **Programmatic control:** You can manipulate DataFrames using Python code, which provides flexibility in handling complex logic, conditional operations, and iterative processes that may not be easily expressed in SQL.\n",
    "- **Easier to maintain:** DataFrames are easier to maintain than SQL queries because they are written in Python code, which is more readable and less error-prone than SQL.\n",
    "- **Debugging:** Python code provides type safety and easier debugging capabilities compared to SQL queries. Errors in Python code are typically caught at runtime, whereas errors in SQL may sometimes only be caught during query execution, leading to potentially faster iteration and debugging cycles.\n",
    "- **Custom functions:** You can define custom Python functions (UDFs - User Defined Functions) within PySpark DataFrames, enabling you to encapsulate business logic and reuse it across different parts of your data pipeline.\n",
    "\n",
    "\n",
    "Nevertheless, we will see how some spark DataFrame methods can be translated into SQL queries, just to show how similar they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Data\n",
    "\n",
    "Now we're going to go through some [Pyspark DataFrame methods](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html) and [PySpark SQL functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html).\n",
    "\n",
    "For that, we'll be using a dataset with five years of customer orders information, including thousands of products sold. The records comprise requests from 2017 to 2021. The data was adapted from the JMP Case Study Library and was obtained from [Kaggle](https://www.kaggle.com/datasets/gabrielsantello/wholesale-and-retail-orders-dataset).\n",
    "\n",
    "Our goal is to perform some data analysis using this dataset. For that, we'll first go through these steps:\n",
    "1. Data Cleaning\n",
    "2. Feature Engineering\n",
    "3. Data Analytics\n",
    "\n",
    "In this notebook we'll cover the first two steps. The third step will be covered in the next two notebooks.\n",
    "\n",
    "Let's start by downloading the data from a downloadable link using the `wget` command we explored earlier. Since the file we're downloading is a zip file, it is also necessary to unzip it.\n",
    "\n",
    "![Retail Orders](https://www.smesouthafrica.co.za/wp-content/uploads/2020/10/DPO-SA--1024x640.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "wget https://github.com/inesmcm26/lp-big-data/raw/main/data/orders-data.zip\n",
    "\n",
    "unzip orders-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files inside the zip:\n",
    "- `orders.csv`: A file containing orders information, including purchase date, product ID, price etc.\n",
    "- `product-supplier.csv`: A file containing information about purchased products as well as their supplier.\n",
    "\n",
    "To avoid downloading the files everytime we want to run this notebook, let's save them to the DBFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "cp -r file:/databricks/driver/orders-data/ /FileStore/lp-big-data/orders-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the orders data, load it to a spark DataFrame using one of the methods we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"/FileStore/lp-big-data/orders-data/orders.csv\")\n",
    ")\n",
    "\n",
    "df_orders.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some information about the columns.\n",
    "\n",
    "For that, we can use the `printSchema()` method. This function prints the schema of the DataFrame, including data types and nullable properties of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how many rows the dataset contains by using the `count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a general overview, let's check some statistic about the data. For that we have two methods: `describe()` and `summary()`.\n",
    "\n",
    "Run both and check the differences.\n",
    "\n",
    "***Note:*** Remember spark's lazy evaluation. To visualize the results you need to trigger an action, for example, call the `display()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summary()` method provides a more in-depth statistical analysis, like the quartiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now that we've got to explore a bit our dataset, let's clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark SQL functions module\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Duplicates\n",
    "\n",
    "We don't want duplicate orders records. Drop the duplicate entries in case they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = df_orders.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns\n",
    "\n",
    "The original column names are too long and contain spaces. This makes it harder to use them, so let's standardize them and make them smaller.\n",
    "\n",
    "There are several ways of renaming a DataFrame's columns.\n",
    "\n",
    "1. Using the `select()` with `alias()`.\n",
    "\n",
    "    You can rename the column names by selecting them and giving them a new alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting columns**\n",
    "\n",
    "First let's see the different ways of selecting columns and then we can put everything together.\n",
    "\n",
    "To select columns you have the following options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "df_orders_1=(\n",
    "    df_orders.select(\n",
    "        'Customer ID',\n",
    "        'Customer Status',\n",
    "        'Date Order was placed',\n",
    "        'Delivery Date',\n",
    "        'Order ID',\n",
    "        'Product ID',\n",
    "        'Quantity Ordered',\n",
    "        'Total Retail Price for This Order',\n",
    "        'Cost Price Per Unit')\n",
    ")\n",
    "\n",
    "# 2\n",
    "df_orders_2=(\n",
    "    df_orders.select(\n",
    "        df_orders['Customer ID'],\n",
    "        df_orders['Customer Status'],\n",
    "        df_orders['Date Order was placed'],\n",
    "        df_orders['Delivery Date'],\n",
    "        df_orders['Order ID'],\n",
    "        df_orders['Product ID'],\n",
    "        df_orders['Quantity Ordered'],\n",
    "        df_orders['Total Retail Price for This Order'],\n",
    "        df_orders['Cost Price Per Unit'],\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3\n",
    "df_orders_3=(\n",
    "    df_orders.select(\n",
    "        f.col('Customer ID'),\n",
    "        f.col('Customer Status'),\n",
    "        f.col('Date Order was placed'),\n",
    "        f.col('Delivery Date'),\n",
    "        f.col('Order ID'),\n",
    "        f.col('Product ID'),\n",
    "        f.col('Quantity Ordered'),\n",
    "        f.col('Total Retail Price for This Order'),\n",
    "        f.col('Cost Price Per Unit'),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend using the 3th. It allows you to be explicit about selecting a column and makes it easier to apply `Column` methods like `alias()`.\n",
    "\n",
    "Let's use it to rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_renamed=(\n",
    "    df_orders.select(\n",
    "        f.col('Customer ID').alias('customer_id'),\n",
    "        f.col('Customer Status').alias('customer_status'),\n",
    "        f.col('Date Order was placed').alias('placing_date'),\n",
    "        f.col('Delivery Date').alias('delivery_date'),\n",
    "        f.col('Order ID').alias('order_id'),\n",
    "        f.col('Product ID').alias('product_id'),\n",
    "        f.col('Quantity Ordered').alias('amount'),\n",
    "        f.col('Total Retail Price for This Order').alias('revenue'),\n",
    "        f.col('Cost Price Per Unit').alias('cost_per_unit'),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_orders_renamed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** We can use a SQL query to select and rename columns as well. Let's see how it would be done.\n",
    "\n",
    "First, we need to create a temporary view of the DataFrame. Only then we can run a SQL query on it using the SparkSession `sql()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.createOrReplaceTempView('df_orders_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the query to rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_sql = spark.sql(\n",
    "    '''\n",
    "        SELECT\n",
    "            `Customer ID` AS customer_id,\n",
    "            `Customer Status` AS customer_status,\n",
    "            `Date Order was placed` AS placing_date,\n",
    "            `Delivery Date` AS delivery_date,\n",
    "            `Order ID` AS order_id,\n",
    "            `Product ID` AS product_id,\n",
    "            `Quantity Ordered` AS amount,\n",
    "            `Total Retail Price for This Order` AS revenue,\n",
    "            `Cost Price Per Unit` AS cost_per_unit\n",
    "        FROM\n",
    "            df_orders_view\n",
    "    '''\n",
    ")\n",
    "\n",
    "df_orders_sql.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance, both methods are similar. However, using DataFrame methods is more flexible and easier to maintain.\n",
    "\n",
    "If we wanted to query the resulting DataFrame using SQL again, we would have to create a temporary view of the DataFrame first, as we did before.\n",
    "\n",
    "Instead, we can simply assign the resulting DataFrame to a new variable and continue to use DataFrame methods on that auxiliary variable. This is easier to maintain because it avoids creating temporary views and makes it easier to debug.\n",
    "\n",
    "This becomes more complex as the number of operations increases. Therefore, it is recommended to use DataFrame methods whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the `withColumnRenamed` DataFrame method.\n",
    "\n",
    "    This method receives two arguments:\n",
    "    - The old column name\n",
    "    - The new column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_renamed_2=(\n",
    "    df_orders\n",
    "    .withColumnRenamed('Customer ID', 'customer_id')\n",
    "    .withColumnRenamed('Customer Status', 'customer_status')\n",
    "    .withColumnRenamed('Date Order was placed', 'placing_date')\n",
    "    .withColumnRenamed('Delivery Date', 'delivery_date')\n",
    "    .withColumnRenamed('Order ID', 'order_id')\n",
    "    .withColumnRenamed('Product ID', 'product_id')\n",
    "    .withColumnRenamed('Quantity Ordered', 'amount')\n",
    "    .withColumnRenamed('Total Retail Price for This Order', 'revenue')\n",
    "    .withColumnRenamed('Cost Price Per Unit', 'cost_per_unit')\n",
    ")\n",
    "\n",
    "df_orders_renamed_2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the `toDF()` method. This is the easiest way of doing it when you want to rename all the columns in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_renamed_3 = (\n",
    "    df_orders.toDF(\n",
    "        'customer_id',\n",
    "        'customer_status',\n",
    "        'placing_date',\n",
    "        'delivery_date',\n",
    "        'order_id',\n",
    "        'product_id',\n",
    "        'amount',\n",
    "        'revenue',\n",
    "        'cost_per_unit'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_orders_renamed_3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and drop columns\n",
    "\n",
    "Have you noticed that the `customer_status` has some values in lower case and others in upper case? We should deal with this issue and make them all lower case.\n",
    "\n",
    "Let's check how many times that happens. We can use the `groupBy` together with the `count` aggregation method to see how many times each unique value appears int he `customer_status` column.\n",
    "\n",
    "We'll see how the `groupBy` works in more detail later. For now let's just use it to mimic the pandas dataframe `value_counts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_renamed.groupBy('customer_status').count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we should only have 3 unique values: Platinum, Silver and Gold, but insetad we also have records with those values in upper case.\n",
    "\n",
    "To standardize them and eliminate these redundancies we can create a new column in the DataFrame using the `withColumn` method. This new column will be obtained by applying a function to the old column. In this case, we want to apply the `lower` function from PySpark SQL functions.\n",
    "\n",
    "Once the new column is created we need to drop the old one. For that we can use the `drop` method.\n",
    "\n",
    "Finally, we can rename the new column to match the old column's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_standardized = (\n",
    "    df_orders_renamed\n",
    "    .withColumn('customer_status_new', f.lower(df_orders_renamed['customer_status']))\n",
    "    .drop('customer_status')\n",
    "    .withColumnRenamed('customer_status_new', 'customer_status')\n",
    ")\n",
    "\n",
    "df_orders_standardized.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again the new unique values on the `customer_status` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_standardized.groupBy('customer_status').count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null values\n",
    "\n",
    "Have you noticed that we have some columns with null values? Let's check the `describe` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_standardized.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns except for `placing_date` and `amount` have 185013 values.\n",
    "\n",
    "We need to deal with these missing values before we proceed with our analysis.\n",
    "\n",
    "Let's make some assumptions. If the `amount` of product ordered is missing, let's assume only one unit was ordered. Therefore, we'll impute the missing values with value `1`.\n",
    "\n",
    "In the cases with missing `placing_date`, it is really hard to infer the missing values, so let's simply drop these observations from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_cleaned = (\n",
    "    df_orders_standardized\n",
    "    # fillna receives the value to fill the missing entries in the subset of columns\n",
    "    .fillna(1, subset=(['amount']))\n",
    "    # dropna drops all the rows with missing value int the specified subset of columns\n",
    "    .dropna(subset=['placing_date'])\n",
    ")\n",
    "\n",
    "df_orders_cleaned.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We managed to clean our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Now we can perform some feature engineering to add value to our dataset. This will enable us to make a better analysis in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date objects\n",
    "\n",
    "There are some new columns we can create based on the `placing_date` and `delivery_date`.\n",
    "\n",
    "As we've just seen before, we can create these new columns by applying functions to the old ones. In this case we want to apply the `to_date` function from PySpark SQL functions.\n",
    "\n",
    "This function receives a column and a date format and converts the string to a `DateType` data type based on the specified format.\n",
    "\n",
    "But as we've seen in the initial schema, these columns are saved as strings, although they represent dates. So first we need to convert them to dates.\n",
    "\n",
    "Also, there's one more problem we need to solve first. The dates are not only saved as strings, but the months are saved as abbreviations instead of numbers. Therefore, we need an additional extra step to convert these abbreviations to numbers, so that we can apply the `to_date` method.\n",
    "\n",
    "There are several ways of doing this, but the most straightforward one is to use the `regexp_replace` function in the PySpark SQL functions library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity we can create a mapping between strings and numbers\n",
    "months_mapping = {\n",
    "    'Jan': '01',\n",
    "    'Feb': '02',\n",
    "    'Mar': '03',\n",
    "    'Apr': '04',\n",
    "    'May': '05',\n",
    "    'Jun': '06',\n",
    "    'Jul': '07',\n",
    "    'Aug': '08',\n",
    "    'Sep': '09',\n",
    "    'Oct': '10',\n",
    "    'Nov': '11',\n",
    "    'Dec': '12'\n",
    "}\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "df_orders_date = df_orders_cleaned.alias('df_orders_date')\n",
    "\n",
    "# Loop through the months and replace the abbreviations with the corresponding values\n",
    "for abbr, nr in months_mapping.items():\n",
    "    df_orders_date = (\n",
    "        df_orders_date\n",
    "        .withColumn('placing_date', f.regexp_replace('placing_date', abbr, nr))\n",
    "        .withColumn('delivery_date', f.regexp_replace('delivery_date', abbr, nr))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_date.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to convert the strings to dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_date_format = (\n",
    "    df_orders_date\n",
    "    .withColumn('placing_date', f.to_date(f.col('placing_date'), 'dd-MM-yy'))\n",
    "    .withColumn('delivery_date', f.to_date(f.col('delivery_date'), 'dd-MM-yy'))\n",
    ")\n",
    "\n",
    "df_orders_date_format.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our date columns in the correct format, we can think of new columns we may engineer.\n",
    "\n",
    "I have prepared some suggestions:\n",
    "\n",
    "- Difference between order and delivery dates: `datediff(end, start)`\n",
    "- Extract the month: `month()`\n",
    "- Extract the year: `year()`\n",
    "- Extract the day of week: `dayofweek()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_date_engineered = (\n",
    "    df_orders_date_format\n",
    "    .withColumn('days_to_delivery', f.datediff(f.col('delivery_date'), f.col('placing_date')))\n",
    "    .withColumn('order_month', f.month(f.col('placing_date')))\n",
    "    .withColumn('order_year', f.year(f.col('placing_date')))\n",
    "    .withColumn('order_day_of_week', f.dayofweek(f.col('placing_date')))\n",
    ")\n",
    "\n",
    "df_orders_date_engineered.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features\n",
    "\n",
    "There are some other features we can add to our dataset, besides the ones related to the dates.\n",
    "\n",
    "Here are some suggestions:\n",
    "\n",
    "- **Order Profit**: Subtract the total cost (cost_per_unit * amount) from the revenue to calculate the profit made from each order. This can help in analyzing profitability.\n",
    "\n",
    "- **Delivery Speed**: We can define that an order was delivered 'Fast' if the delivery happened in 1 day of less, 'Medium' if it was between 1-3 days and 'Slow' if higher than that. For that we can use the `when(condition, value)` function.\n",
    "\n",
    "Let's create them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_engineered = (\n",
    "    df_orders_date_engineered\n",
    "    .withColumn('profit', f.col('revenue') - (f.col('cost_per_unit') * f.col('amount')))\n",
    "    .withColumn('delivery_speed',\n",
    "                f.when(f.col('days_to_delivery') <= 1, 'Fast')\n",
    "                .when(f.col('days_to_delivery') <= 3, 'Medium')\n",
    "                .otherwise('Slow'))\n",
    "    \n",
    ")\n",
    "\n",
    "df_orders_engineered.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We managed to create some new features that will be valuable for later analysis.\n",
    "\n",
    "Let's save the preprocessed orders dataset to the DBFS to avoid running the preprocessing steps all over again.\n",
    "\n",
    "Save it as a csv file to `FileStore/lp-big-data/preprocessed-data/orders-data/` with the name `orders-preprocessed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_engineered.write.csv('/FileStore/lp-big-data/orders-data/orders-preprocessed.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to apply what we've learned so far. Go to the `exercises-part1` notebook and try to solve the exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daredata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
