{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    MapType,\n",
    "    FloatType,\n",
    "    DateType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex data types - Part II\n",
    "\n",
    "In the previous notebook, we explored PySpark's complex data types and some basic functions. In this notebook, we will continue exploring some more functions, this time more complex ones.\n",
    "\n",
    "First, let's start by creating a sample dataset with information about orders from a Fruit Shop.\n",
    "\n",
    "This dataset may remind you of the one used for the exercises of Part I, but this time we will be able to perform much more complex operations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    {\"order_id\": 5642, \"order_date\": datetime.strptime(\"2024-05-18\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"amount\": 1.0, \"unit_price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"amount\": 1.7, \"unit_price\": 1.99}],\n",
    "    'items_discount': ['Apple']},\n",
    "    {\"order_id\": 9762, \"order_date\": datetime.strptime(\"2024-05-02\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"amount\": 0.5, \"unit_price\": 6.99},\n",
    "        {\"name\": \"Apple\", \"amount\": 3.0, \"unit_price\": 2.99},\n",
    "        {\"name\": \"Peach\", \"amount\": 2.5, \"unit_price\": 3.39}],\n",
    "    'items_discount': ['Apple', 'Peach']},\n",
    "    {\"order_id\": 3652, \"order_date\": datetime.strptime(\"2024-05-23\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"amount\": 1.5, \"unit_price\": 1.99}],\n",
    "    'items_discount': []},\n",
    "    {\"order_id\": 1276, \"order_date\": datetime.strptime(\"2024-05-10\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"amount\": 2.0, \"unit_price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"amount\": 0.5, \"unit_price\": 1.99},\n",
    "        {\"name\": \"Strawberry\", \"amount\": 1.0, \"unit_price\": 6.99},\n",
    "        {\"name\": \"Strawberry\", \"amount\": 1.0, \"unit_price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"amount\": 1.0, \"unit_price\": 3.39}],\n",
    "    'items_discount': ['Peach', 'Banana']},\n",
    "    {\"order_id\": 8763, \"order_date\": datetime.strptime(\"2024-05-14\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"amount\": 1.0, \"unit_price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"amount\": 1.0, \"unit_price\": 3.39},\n",
    "        {\"name\": \"Mango\", \"amount\": 1.5, \"unit_price\": 5.99}],\n",
    "    'items_discount': ['Mango']},\n",
    "    {\"order_id\": 7652, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"amount\": 1.0, \"unit_price\": 1.99},\n",
    "        {\"name\": \"Mango\", \"amount\": 1.5, \"unit_price\": 5.99}],\n",
    "    'items_discount': ['Mango', 'Banana']},\n",
    "    {\"order_id\": 7631, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"amount\": 1.0, \"unit_price\": 1.99},\n",
    "        {\"name\": \"Banana\", \"amount\": 2.5, \"unit_price\": 1.99},],\n",
    "    'items_discount': []}\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType(), False),\n",
    "    StructField('order_date', DateType(), False),\n",
    "    StructField(\n",
    "        'items',\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField('name', StringType(), False),\n",
    "                StructField('amount', FloatType(), False),\n",
    "                StructField('unit_price', FloatType(), False)\n",
    "            ]),\n",
    "            False\n",
    "        ),\n",
    "        False\n",
    "    ),\n",
    "    StructField(\"items_discount\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_fruitshop = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df_fruitshop.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `items` column has a complex data type, the `ArrayType`.\n",
    "\n",
    "The arrays that form this column hold elements of other complex type, the `StructType`, which are composed by a list of `StructField` elements.\n",
    "\n",
    "These elements have names `name`, `unit_price` and `amount`, with types `StringType`, `DoubleType`and `DoubleType`, and none of them can have NULL values.\n",
    "\n",
    "The `items_disount` column is simply an array of strings.\n",
    "\n",
    "The next sections will show how to use some functions to manipulate these complex data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "The [transform()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.transform.html) function applies a function to each element of an array column.\n",
    "\n",
    "Let's say we want to get the name of each product in the `items` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform function. This receives a column and a function to apply to the column.\n",
    "fn = f.transform(f.col('items'), lambda x: x['name'])\n",
    "\n",
    "df_fruitshop_transformed = (\n",
    "    df_fruitshop\n",
    "    # Apply the transformation to the dataframe and save the results to a new column\n",
    "    .withColumn('item_names', fn)\n",
    ")\n",
    "\n",
    "df_fruitshop_transformed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema of the new DataFrame to see the type of column `item_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `item_names` column is an array of strings, where each string is the name of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array distinct\n",
    "\n",
    "The [array_distinct()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array_distinct.html) function returns an array of distinct elements in the input array.\n",
    "\n",
    "Let's get the list of unique items in each order, as well as the number of unique items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_unique = (\n",
    "    df_fruitshop_size\n",
    "    .withColumn(\n",
    "        'unique_item_names',\n",
    "        f.array_distinct(f.col('item_names'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'nr_unique_items',\n",
    "        f.size(f.col('unique_item_names'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_unique.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array contains\n",
    "\n",
    "The [array_contains()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array_contains.html) function checks if an array contains a specific value.\n",
    "\n",
    "Create a new column of boolean values indicating if the order contains the `Banana` item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_contains = (\n",
    "    df_fruitshop_unique\n",
    "    .withColumn(\n",
    "        'contains_banana',\n",
    "        f.array_contains(f.col('unique_item_names'), 'Banana')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_contains.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this function returns a column of boolean values, we could also use it to filter the DataFrame and keep only the orders in which `Banana` was purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_filtered = (\n",
    "    df_fruitshop_unique\n",
    "    .filter(\n",
    "        f.array_contains(f.col('unique_item_names'), 'Banana')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_filtered.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode\n",
    "\n",
    "The [explode()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.explode.html) function creates a new row for each element in an array or map.\n",
    "\n",
    "Let's try it out. Transform the original DataFrame to get a new DataFrame with one row for each unique item purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_exploded = (\n",
    "    df_fruitshop_unique\n",
    "    .select(\n",
    "        'order_id',\n",
    "        'order_date',\n",
    "        f.explode(f.col('unique_item_names')).alias('item')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_exploded.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect list\n",
    "\n",
    "The [collect_list()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.collect_list.html) function  is an aggregate function that can be applied over a [Window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html) or using the [groupBy()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html) method.\n",
    "\n",
    "It allows you to collect the results of an aggregation into an array.\n",
    "\n",
    "Let's revert the `explode()` operation and collect the list of unique items purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_collected = (\n",
    "    df_fruitshop_exploded\n",
    "    .groupBy('order_id', 'order_date')\n",
    "    .agg(\n",
    "        f.collect_list(f.col('item')).alias('unique_item_names')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline\n",
    "\n",
    "The [inline()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.inline.html) function explodes an array of structs into a DataFrame.\n",
    "\n",
    "Let's say we want to create a new table with the name, unit_price and amount of each item purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_fruitshop_unique\n",
    "    .select(\n",
    "        'order_id',\n",
    "        f.inline(f.col('items'))\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `explode` function to see the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_fruitshop_unique\n",
    "    .select(\n",
    "        'order_id',\n",
    "        f.explode(f.col('items'))\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that `explode` creates a new row for each element in the array, while `inline` not only creates a new row for each element in the array but also creates a new column for each field in the struct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "\n",
    "There are a lot more collection functions you can use with complex types, like:\n",
    "\n",
    "- `concat()`: Concatenate multiple arrays.\n",
    "- `arrays_zip()`: Combine multiple arrays into a single array of structs.\n",
    "- `array_except()`: Return an array containing elements from the first array that are not present in the second array.\n",
    "- `array_intersect()`: Return an array containing elements that are present in both input arrays.\n",
    "- `array_union()`: Return an array containing elements from both input arrays, without duplicates.\n",
    "- `array_sort()`:Sort the elements of an array in ascending order.\n",
    "- `array_max()`: Find the maximum value in an array.\n",
    "- `array_position()`: Find the position (index) of a specific value in an array.\n",
    "- `flatten()`: Flatten a nested array structure.\n",
    "- `map_filter(col, f)`: Returns a map whose key-value pairs satisfy a predicate.\n",
    "\n",
    "etc.\n",
    "\n",
    "A complete list of functions can be found in the [PySpark SQL Functions documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#collection-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we learned about complex data types in PySpark, such as ArrayType, StructType, and MapType.\n",
    "\n",
    "Go to the `exercises` notebook to practice what you've learned!\n",
    "\n",
    "But before that, save the `df_fruitshop` DataFrame to the a csv file in the dbfs to use it in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop.write.parquet('/FileStore/lp-big-data/fruitshop.parquet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
