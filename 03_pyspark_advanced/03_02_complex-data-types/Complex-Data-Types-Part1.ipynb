{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    MapType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Data Types - Part I\n",
    "\n",
    "In this notebook, we will learn about complex data types in Python.\n",
    "\n",
    "Until now, we have worked with simple data types like integers, floats, and strings. But in real-world applications, we often need to work with more complex data types.\n",
    "\n",
    "These complex data types are usually composed by combining multiple simple data types together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Type\n",
    "\n",
    "An array is an ordered collection of elements of the same type. It is useful for representing a collection of values that are related in some way.\n",
    "\n",
    "Elements in an array can be accessed using an index.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [ArrayType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.ArrayType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample data\n",
    "data = [(\"Sara\", [\"Portugal\", \"Spain\", \"France\"], 1, 2),\n",
    "        (\"John\", [\"UK\", \"Belgium\", \"Netherlands\", \"Denmark\"], 2, 1),\n",
    "        (\"Steffano\", [\"Italy\", \"Croatia\", \"Switzerland\", \"Portugal\", \"UK\"], 2, 5),\n",
    "        (\"Marina\", [\"Spain\", \"Portugal\", \"France\", \"UK\"], 4, 1)]\n",
    "\n",
    "# Define a schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"visited_countries\", ArrayType(StringType()), True),\n",
    "    StructField(\"best_idx\", IntegerType()),\n",
    "    StructField(\"worst_idx\", IntegerType())\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the column `visited_countries` is an Array column with elements of type String. This column contains the countries visited by each person, ordered by the visitation date.\n",
    "\n",
    "The columns `best_idx` and `worst_idx` indicate the index of the best and worst countries visited by each person, respectively, on the `visited_countries` array.\n",
    "\n",
    "As an introduction, let's explore some simple functions we can apply to `ArrayType` columns.\n",
    "\n",
    "We are going to create two new columns with the best and worst country names. We need to access the values in `visited_countries` using the `best_idx` and `worst_idx`.\n",
    "\n",
    "For that we'll use the PySpark SQL [element_at()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.element_at.html) function, which receives an Array column and an index (which can also be a column of indexes). \n",
    "\n",
    "***Note:*** In Spark, the first element of an array has index 1, not 0 like in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_worst = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        'best_country',\n",
    "        f.element_at(f.col('visited_countries'), f.col('best_idx'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'worst_country',\n",
    "        f.element_at(f.col('visited_countries'), f.col('worst_idx'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_best_worst.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the second and third countries visited by each person.\n",
    "\n",
    "For that, we need to slice the `visited_countries` array from index 2 to index 3, using the [slice()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.slice.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2nd_3rd = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        '2nd_3rd_countries',\n",
    "        f.slice(f.col('visited_countries'), start=2, length=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "df_2nd_3rd.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many countries each person visited using the [size()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.size.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nr_countries = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        'nr_countries',\n",
    "        f.size(f.col('visited_countries'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_nr_countries.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's collapse the best and worst countries into a single Array column\n",
    "\n",
    "For that we need to create an array using the [array()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array.html) function, and assign it to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_best_worst\n",
    "    .withColumn(\n",
    "        'best_worst_countries',\n",
    "        f.array(f.col('best_country'), f.col('worst_country'))\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several more functions that can be applied to arrays. We'll see them in more detail on the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Type\n",
    "\n",
    "A column of type `StructType` represents a structured record with a fixed set of fields.\n",
    "\n",
    "Each field inside a Struct has type [StructField](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructField.html#pyspark.sql.types.StructField).\n",
    "\n",
    "Structs are useful for representing structured data where each field has a name and a type. Fields in a struct can be accessed by their name.\n",
    "\n",
    "In PySpark, structs are commonly used to represent nested or hierarchical data structures within a DataFrame. As you may recall, we use this data type to define the schema of a DataFrame when loading data from a file.\n",
    "\n",
    "You can think of Structs as Python dictionaries, where the dictionary keys and value types are predefined and are the same for all records.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [StructType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.StructType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample\n",
    "data = [(1, (\"Sara\", 30, \"Female\")),\n",
    "        (2, (\"John\", 35, \"Male\")),\n",
    "        (3, (\"Steffano\", 40, \"Male\")),\n",
    "        (4, (\"Marina\", 20, \"Female\"))]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    # Define the struct fields\n",
    "    StructField(\"person_id\", IntegerType(), False),\n",
    "    StructField(\n",
    "        \"person_info\",\n",
    "        # A struct field can also have type StructType\n",
    "        StructType([\n",
    "            # Which in turn contains struct fields\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"gender\", StringType(), True)\n",
    "        ]),\n",
    "        False\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the schema, `person_info` is of type `StructType`. This struct has one field for the name, other for the age, and other for the gender of the person.\n",
    "\n",
    "Let's get each person's name, age and gender in separate columns. We can do this by getting the field values from the struct using the [getField()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.getField.html) column function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_columns = (\n",
    "    df\n",
    "    .select(\n",
    "        f.col('person_id'),\n",
    "        f.col('person_info').getField('name').alias('name'),\n",
    "        f.col('person_info').getField('age').alias('age'),\n",
    "        f.col('person_info').getField('gender').alias('gender')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_new_columns.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the field values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_columns = (\n",
    "    df\n",
    "    .select(\n",
    "        'person_id',\n",
    "        'person_info.name',\n",
    "        'person_info.age',\n",
    "        'person_info.gender',\n",
    "    )\n",
    ")\n",
    "\n",
    "df_new_columns.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the process and create the struct column again using the [struct()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.struct.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct = (\n",
    "    df_new_columns\n",
    "    .select(\n",
    "        f.col('person_id'),\n",
    "        f.struct('name','age','gender').alias('person_info')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_struct.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Type\n",
    "\n",
    "A Map is an unordered collection of key-value pairs. It is useful for representing a collection of values that are indexed by a key.\n",
    "\n",
    "The Map data type also resembles a Python dictionary, where each key is associated with a value.\n",
    "\n",
    "Unlike structs, maps are not limited to a fixed set of fields and their keys and values may vary across rows.\n",
    "\n",
    "All keys in a map are of the same type, and all values are of the same type as well.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [MapType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.MapType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample data\n",
    "data = [(\"Sara\", {\"Maths\": 90, \"Science\": 75, \"History\": 95}),\n",
    "        (\"John\", {\"Biology\": 85, \"Maths\": 75, \"Chemistry\": 88, \"Literature\": 75}),\n",
    "        (\"Stefanno\", {\"Maths\": 90, \"Science\": 75, \"Economy\": 85}),\n",
    "        (\"Marina\", {\"Science\": 68, \"History\": 100})]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"grades\", MapType(StringType(), IntegerType()), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has one column `name` and another `grades` of type `MapType`, which contains the grades for different subjects. Since different students may have taken different subjects, it makes sense to use a `MapType` to save this data instead of a `StructType`.\n",
    "\n",
    "The grades are saved as values between 0 and 100. Let's convert them to values between 0 and 20.\n",
    "\n",
    "To do this, we can transform the values in a Map column using the [transform_values()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.transform_values.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.withColumn('transformed_grades', f.transform_values('grades', lambda k, v: v*20/100))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the subjects where the grade is bigger than 80 and save their grades in a new column using the [map_filter()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_filter.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.withColumn('grades_above_80', f.map_filter('grades', lambda k, v: v > 80))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we learned about complex data types in PySpark, such as ArrayType, StructType, and MapType.\n",
    "\n",
    "Go to the `exercises-part1` notebook to practice what you've learned!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
