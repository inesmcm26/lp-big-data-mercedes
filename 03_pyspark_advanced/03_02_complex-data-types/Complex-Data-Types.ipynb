{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    MapType,\n",
    "    FloatType,\n",
    "    DateType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Data Types\n",
    "\n",
    "In this notebook, we will learn about complex data types in Python.\n",
    "\n",
    "Until now, we have worked with simple data types like integers, floats, and strings. But in real-world applications, we often need to work with more complex data types.\n",
    "\n",
    "These complex data types are usually composed by combining multiple simple data types together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Type\n",
    "\n",
    "An array is an ordered collection of elements of the same type. It is useful for representing a collection of values that are related in some way.\n",
    "\n",
    "Elements in an array can be accessed using an index.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [ArrayType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.ArrayType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample data\n",
    "data = [(\"Sara\", [\"Portugal\", \"Spain\", \"France\"], 1, 2),\n",
    "        (\"John\", [\"UK\", \"Belgium\", \"Netherlands\", \"Denmark\"], 2, 1),\n",
    "        (\"Steffano\", [\"Italy\", \"Croatia\", \"Switzerland\", \"Portugal\", \"UK\"], 2, 5),\n",
    "        (\"Marina\", [\"Spain\", \"Portugal\", \"France\", \"UK\"], 4, 1)]\n",
    "\n",
    "# Define a schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"visited_countries\", ArrayType(StringType()), True),\n",
    "    StructField(\"best_idx\", IntegerType()),\n",
    "    StructField(\"worst_idx\", IntegerType())\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the column `visited_countries` is an Array column with elements of type String. This column contains the countries visited by each person, ordered by the visitation date.\n",
    "\n",
    "The columns `best_idx` and `worst_idx` indicate the index of the best and worst countries visited by each person, respectively, on the `visited_countries` array.\n",
    "\n",
    "As an introduction, let's explore some simple functions we can apply to `ArrayType` columns.\n",
    "\n",
    "We are going to create two new columns with the best and worst country names. We need to access the values in `visited_countries` using the `best_idx` and `worst_idx`.\n",
    "\n",
    "For that we'll use the PySpark SQL [element_at()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.element_at.html) function, which receives an Array column and an index (which can also be a column of indexes). \n",
    "\n",
    "***Note:*** In Spark, the first element of an array has index 1, not 0 like in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_worst = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        'best_country',\n",
    "        f.element_at(f.col('visited_countries'), f.col('best_idx'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'worst_country',\n",
    "        f.element_at(f.col('visited_countries'), f.col('worst_idx'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_best_worst.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the second and third countries visited by each person.\n",
    "\n",
    "For that, we need to slice the `visited_countries` array from index 2 to index 3, using the [slice()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.slice.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2nd_3rd = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        '2nd_3rd_countries',\n",
    "        f.slice(f.col('visited_countries'), start=2, length=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "df_2nd_3rd.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's collapse the best and worst countries into a single Array column\n",
    "\n",
    "For that we need to create an array using the [array()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array.html) function, and assign it to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_best_worst\n",
    "    .withColumn(\n",
    "        'best_worst_countries',\n",
    "        f.array(f.col('best_country'), f.col('worst_country'))\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several more functions that can be applied to arrays. We'll see them in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Type\n",
    "\n",
    "A Struct represents a structured record with a fixed set of fields. These fields are of type [StructField](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructField.html#pyspark.sql.types.StructField), which is a complex data type itself.\n",
    "\n",
    "Structs are useful for representing structured data where each field has a name and a type. Fields in a struct can be accessed by their name.\n",
    "\n",
    "In PySpark, structs are commonly used to represent nested or hierarchical data structures within a DataFrame. As you may recall, we use this data type to define the schema of a DataFrame when loading data from a file.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [StructType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.StructType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample\n",
    "data = [(1, (\"Sara\", 30, \"Female\")),\n",
    "        (2, (\"John\", 35, \"Male\")),\n",
    "        (3, (\"Steffano\", 40, \"Male\")),\n",
    "        (4, (\"Marina\", 20, \"Female\"))]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    # Define the struct fields\n",
    "    StructField(\"person_id\", IntegerType(), False),\n",
    "    StructField(\n",
    "        \"person_info\",\n",
    "        # A struct field can also have type StructType\n",
    "        StructType([\n",
    "            # Which in turn contains struct fields\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"gender\", StringType(), True)\n",
    "        ]),\n",
    "        False\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the schema, `person_info` is of type `StructType`. This struct has one field for the name, other for the age, and other for the gender of the person.\n",
    "\n",
    "Let's get each person's name, age and gender in separate columns. We can do this by getting the field values from the struct using the [getField()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.getField.html) column function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_columns = (\n",
    "    df\n",
    "    .select(\n",
    "        f.col('person_id'),\n",
    "        f.col('person_info').getField('name').alias('name'),\n",
    "        f.col('person_info').getField('age').alias('age'),\n",
    "        f.col('person_info').getField('gender').alias('gender')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_new_columns.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the field values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_columns = (\n",
    "    df\n",
    "    .select(\n",
    "        'person_id',\n",
    "        'person_info.name',\n",
    "        'person_info.age',\n",
    "        'person_info.gender',\n",
    "    )\n",
    ")\n",
    "\n",
    "df_new_columns.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the process and create the struct column again using the [struct()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.struct.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct = (\n",
    "    df_new_columns\n",
    "    .select(\n",
    "        f.col('person_id'),\n",
    "        f.struct(\n",
    "            f.col('name'),\n",
    "            f.col('age'),\n",
    "            f.col('gender')\n",
    "        ).alias('person_info')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_struct.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Type\n",
    "\n",
    "A Map is an unordered collection of key-value pairs. It is useful for representing a collection of values that are indexed by a key.\n",
    "\n",
    "The Map data type resembles a Python dictionary, where each key is associated with a value.\n",
    "\n",
    "Unlike structs, maps are not limited to a fixed set of fields and their keys and values may vary across rows.\n",
    "\n",
    "All keys in a map are of the same type, and all values are of the same type as well.\n",
    "\n",
    "Let's create a PySpark DataFrame with a column of values belonging to [MapType](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.MapType.html) to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample data\n",
    "data = [(\"Sara\", {\"Maths\": 90, \"Science\": 75, \"History\": 95}),\n",
    "        (\"John\", {\"Biology\": 85, \"Maths\": 75, \"Chemistry\": 88, \"Literature\": 75}),\n",
    "        (\"Stefanno\", {\"Maths\": 90, \"Science\": 75, \"Economy\": 85}),\n",
    "        (\"Marina\", {\"Science\": 68, \"History\": 100})]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"grades\", MapType(StringType(), IntegerType()), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has one column `name` and another `grades` of type `MapType`, which contains the grades for different subjects. Since different students may have taken different subjects, it makes sense to use a `MapType` to save this data instead of a `StructType`.\n",
    "\n",
    "The grades are saved as values between 0 and 100. Let's convert them to values between 0 and 20.\n",
    "\n",
    "To do this, we can transform the values in a Map column using the [transform_values()](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.transform_values.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\n",
    "        f.col('name'),\n",
    "        f.transform_values('grades', lambda k, v: v*20/100).alias('transformed_grades')\n",
    "        )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the subjects where the grade is bigger than 80 and save their grades in a new column using the [map_filter()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.map_filter.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .select(\n",
    "        f.col('name'),\n",
    "        f.col('grades'),\n",
    "        f.map_filter('grades', lambda k, v: v > 80).alias('grades_above_80')\n",
    "        )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex data types functions\n",
    "\n",
    "Now that we've explored PySpark's complex data types and some basic functions that can be used with each, let's explore some more functions with a concrete example.\n",
    "\n",
    "First, let's start by creating a sample dataset with information about orders from a Fruit Shop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    {\"order_id\": 5642, \"order_date\": datetime.strptime(\"2024-05-18\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"quantity\": 1.0, \"price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.7, \"price\": 1.99}],\n",
    "    'items_discount': ['Apple']},\n",
    "    {\"order_id\": 9762, \"order_date\": datetime.strptime(\"2024-05-02\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 0.5, \"price\": 6.99},\n",
    "        {\"name\": \"Apple\", \"quantity\": 3.0, \"price\": 2.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 2.5, \"price\": 3.39}],\n",
    "    'items_discount': ['Apple', 'Peach']},\n",
    "    {\"order_id\": 3652, \"order_date\": datetime.strptime(\"2024-05-23\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.5, \"price\": 1.99}],\n",
    "    'items_discount': []},\n",
    "    {\"order_id\": 1276, \"order_date\": datetime.strptime(\"2024-05-10\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"quantity\": 2.0, \"price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 0.5, \"price\": 1.99},\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 1.0, \"price\": 3.39}],\n",
    "    'items_discount': ['Peach', 'Banana']},\n",
    "    {\"order_id\": 8763, \"order_date\": datetime.strptime(\"2024-05-14\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 1.0, \"price\": 3.39},\n",
    "        {\"name\": \"Mango\", \"quantity\": 1.5, \"price\": 5.99}],\n",
    "    'items_discount': ['Mango']},\n",
    "    {\"order_id\": 7652, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.0, \"price\": 1.99},\n",
    "        {\"name\": \"Mango\", \"quantity\": 1.5, \"price\": 5.99}],\n",
    "    'items_discount': ['Mango', 'Banana']},\n",
    "    {\"order_id\": 7631, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.0, \"price\": 1.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 2.5, \"price\": 1.99},],\n",
    "    'items_discount': []}\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType(), False),\n",
    "    StructField('order_date', DateType(), False),\n",
    "    StructField(\n",
    "        'items',\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField('name', StringType(), False),\n",
    "                StructField('quantity', FloatType(), False),\n",
    "                StructField('price', FloatType(), False)\n",
    "            ]),\n",
    "            False\n",
    "        ),\n",
    "        False\n",
    "    ),\n",
    "    StructField(\"items_discount\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_fruitshop = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df_fruitshop.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `items` column has a complex data type, the `ArrayType`.\n",
    "\n",
    "The arrays that form this column hold elements of other complex type, the `StructType`, which are composed by a list of `StructField` elements. These elements have names `name`, `price` and `quantity`, types `StringType`, `DoubleType`and `DoubleType`, and nullable property `True`, `True` and `True`, respectively.\n",
    "\n",
    "The `items_disount` column is simply an array of strings.\n",
    "\n",
    "The next sections will show how to use some functions to manipulate these complex data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "The [transform()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.transform.html) function applies a function to each element of an array column.\n",
    "\n",
    "Let's say we want to get the name of each product in the `items` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform function. This receives a column and a function to apply to the column.\n",
    "fn = f.transform(f.col('items'), lambda x: x.name)\n",
    "\n",
    "df_fruitshop_transformed = (\n",
    "    df_fruitshop\n",
    "    # Apply the transformation to the dataframe and save the results to a new column\n",
    "    .withColumn('item_names', fn)\n",
    ")\n",
    "\n",
    "df_fruitshop_transformed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema of the new DataFrame to see the type of column `item_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `item_names` column is an array of strings, where each string is the name of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size\n",
    "\n",
    "The [size()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.size.html) function returns the size of an array or map.\n",
    "\n",
    "Let's say we want to know the number of items in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_size = (\n",
    "    df_fruitshop_transformed\n",
    "    .withColumn(\n",
    "        'nr_items',\n",
    "        f.size(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_size.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array distinct\n",
    "\n",
    "The [array_distinct()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array_distinct.html) function returns an array of distinct elements in the input array.\n",
    "\n",
    "Let's get the list of unique items in each order, as well as the number of unique items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_unique = (\n",
    "    df_fruitshop_size\n",
    "    .withColumn(\n",
    "        'unique_item_names',\n",
    "        f.array_distinct(f.col('item_names'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'nr_unique_items',\n",
    "        f.size(f.col('unique_item_names'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_unique.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array contains\n",
    "\n",
    "The [array_contains()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.array_contains.html) function checks if an array contains a specific value.\n",
    "\n",
    "Create a new column of boolean values indicating if the order contains the `Banana` item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_contains = (\n",
    "    df_fruitshop_unique\n",
    "    .withColumn(\n",
    "        'contains_banana',\n",
    "        f.array_contains(f.col('unique_item_names'), 'Banana')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_contains.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this function returns a column of boolean values, we could also use it to filter the DataFrame and keep only the orders in which `Banana` was purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_filtered = (\n",
    "    df_fruitshop_unique\n",
    "    .filter(\n",
    "        f.array_contains(f.col('unique_item_names'), 'Banana')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_filtered.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode\n",
    "\n",
    "The [explode()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.explode.html) function creates a new row for each element in an array or map.\n",
    "\n",
    "Let's try it out. Transform the original DataFrame to get a new DataFrame with one row for each unique item purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_exploded = (\n",
    "    df_fruitshop_contains\n",
    "    .select(\n",
    "        'order_id',\n",
    "        'order_date',\n",
    "        f.explode(f.col('unique_item_names')).alias('item')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_exploded.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect list\n",
    "\n",
    "The [collect_list()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.collect_list.html) function  is an aggregate function that can be applied over a [Window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html) or using the [groupBy()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html) method.\n",
    "\n",
    "It allows you to collect the results of an aggregation into an array.\n",
    "\n",
    "Let's revert the `explode()` operation and collect the list of unique items purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_collected = (\n",
    "    df_fruitshop_exploded\n",
    "    .groupBy('order_id', 'order_date')\n",
    "    .agg(\n",
    "        f.collect_list(f.col('item')).alias('unique_item_names')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline\n",
    "\n",
    "The [inline()](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.inline.html) function explodes an array of structs into a DataFrame.\n",
    "\n",
    "Let's say we want to create a new table with the name, price and quantity of each item purchased in each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_fruitshop_filtered\n",
    "    .select(\n",
    "        'order_id',\n",
    "        f.inline(f.col('items'))\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "\n",
    "There are a lot more collection functions you can use with complex types, like:\n",
    "\n",
    "- `concat()`: Concatenate multiple arrays.\n",
    "- `arrays_zip()`: Combine multiple arrays into a single array of structs.\n",
    "- `array_except()`: Return an array containing elements from the first array that are not present in the second array.\n",
    "- `array_intersect()`: Return an array containing elements that are present in both input arrays.\n",
    "- `array_union()`: Return an array containing elements from both input arrays, without duplicates.\n",
    "- `array_sort()`:Sort the elements of an array in ascending order.\n",
    "- `array_max()`: Find the maximum value in an array.\n",
    "- `array_position()`: Find the position (index) of a specific value in an array.\n",
    "- `flatten()`: Flatten a nested array structure.\n",
    "- `map_filter(col, f)`: Returns a map whose key-value pairs satisfy a predicate.\n",
    "\n",
    "etc.\n",
    "\n",
    "A complete list of functions can be found in the [PySpark SQL Functions documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#collection-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we learned about complex data types in PySpark, such as ArrayType, StructType, and MapType.\n",
    "\n",
    "Go to the `exercises` notebook to practice what you've learned!\n",
    "\n",
    "But before that, save the `df_fruitshop` DataFrame to the a csv file in the dbfs to use it in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop.write.parquet('/FileStore/lp-big-data/fruitshop.parquet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
