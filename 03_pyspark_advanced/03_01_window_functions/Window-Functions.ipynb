{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions\n",
    "\n",
    "In this notebook we'll introduce window functions, which are a powerful tool for working with data in SQL.\n",
    "\n",
    "Window functions in PySpark (and in SQL in general) are a tool for performing complex analytical tasks within PySpark DataFrames. They allow you to perform **calculations across a group of rows related to the current row, without aggregating the entire dataset**. This makes them very useful for tasks like calculating moving averages, ranking items within groups, and calculating cumulative sums.\n",
    "\n",
    "**Window functions allow you to calculate aggregate functions (like sum, average, max, min) over a specific window of rows**.\n",
    "\n",
    "Also, using window functions can be more efficient than traditional group by aggregations, especially when you want to include aggregated values alongside individual rows without reducing the number of rows.\n",
    "\n",
    "In this section, we're going to explore the main PySpark window functions. Let's start by downloading a dummy dataset of employees and load it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "wget https://raw.githubusercontent.com/inesmcm26/lp-big-data/main/data/employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employees = (\n",
    "    spark.read.format('csv')\n",
    "    .option('inferSchema', 'true')\n",
    "    .option('header', 'true')\n",
    "    .option('sep', ',')\n",
    "    .load('file:/databricks/driver/employees.csv')\n",
    ")\n",
    "\n",
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Window functions, we first need to define a `Window`. This class is part of the PySpark SQL library and allows you to perform calculations across a group of rows related to one another. [Here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html) you can find the official documentation of this class.\n",
    "\n",
    "It has three main methods:\n",
    "\n",
    "- `partitionBy(col/cols)`: Partitioning is the process of dividing the dataset into groups of rows based on some criteria. This method is used to define these partitions.\n",
    "- `orderBy(col/cols)`: This method defines the order in which rows within each partition are processed. If no partition is defined, the whole dataset is considered.\n",
    "- `rowsBetween(start, end)`: Defines the range of rows included in a window frame for window functions. For each row, the window function will only consider the rows within the specified window range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation functions\n",
    "\n",
    "We've seen some aggregation function in the previous module. These were used to aggregate the values resulting from a `groupBy` operation.\n",
    "\n",
    "However, these functions can also be used with Window operations.\n",
    "\n",
    "Let's answer some questions to exemplify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each employee, what is the average salary in their department?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define our Window object.\n",
    "# We want to partition by 'department'\n",
    "window = Window.partitionBy('department')\n",
    "\n",
    "# Next we define an aggregation function over the window\n",
    "avg_salary_fn = f.avg(f.col('salary')).over(window)\n",
    "\n",
    "# To get the result we need to apply the window function to a dataframe\n",
    "# To do that, we create a new column with the result of the window function\n",
    "df_employees.withColumn('avg_salary_department', avg_salary_fn).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For each employee, what is the maximum salary in their department?\n",
    "\n",
    "We have two ways of answering this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Partition by department\n",
    "window = Window.partitionBy('department')\n",
    "\n",
    "# Define the `max` aggregation function\n",
    "max_salary_fn = f.max('salary').over(window)\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_employees.withColumn('max_salary_department', max_salary_fn).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Partition by department and order the salary values in descending\n",
    "# order within each Window frame\n",
    "window = Window.partitionBy('department').orderBy(f.desc('salary'))\n",
    "\n",
    "# Define the `first` aggregation function to get the first salary\n",
    "# in each window frame\n",
    "max_salary_fn = f.first('salary').over(window)\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_employees.withColumn('max_salary_department', max_salary_fn).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For each employee, what is the average salary per gender in their department?\n",
    "\n",
    "We can also partition the dataset using more than 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define our Window object.\n",
    "# We want to partition by 'department' and 'gender'\n",
    "window = Window.partitionBy(['department', 'gender'])\n",
    "\n",
    "# Next we apply an aggregation fucntion over the window\n",
    "avg_salary_fn = f.avg(f.col('salary')).over(window)\n",
    "\n",
    "# To get the result from the window function we need to apply it to a dataframe\n",
    "df_employees.withColumn('avg_salary_sex_department', avg_salary_fn).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know what you're thinking. The way we answered these questions is very similar to using a `groupBy` operation. In fact, in questions ***1*** and ***2***, for instance, all employees from the same department have the same aggregated value.\n",
    "\n",
    "So, why use window functions? Mainly because the questions started with 'For each employee...', which makes it useful to have a new column with the aggregated value for each employee.\n",
    "\n",
    "If, for example, question ***1*** was 'What is the average salary in each department?', then a `groupBy` operation would be enough.\n",
    "\n",
    "\n",
    "As you can see, sometimes it might be useful to maintain the dataset shape as is. Window functions enable you to compute aggregated values alongside detailed information from other columns in the dataset. This allows for more complex analyses and reporting scenarios where you need both aggregated summaries and detailed information.\n",
    "\n",
    "Let's see a more complex example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Imagine you want to calculate the difference between each employee's salary and the average salary of the five employees closest in salary, including the employee itself (two with higher salaries and two with lower salaries)\n",
    "\n",
    "Let's break this question down:\n",
    "- We want to calculate the difference between each employee's salary and an average salary\n",
    "- The average salary should be calculated considering only 5 employees' salaries\n",
    "- These 5 employees are the two above in terms of salary order, the two below, and the employee himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window with a limited size: between the 2 before and 2 after rows\n",
    "window = Window.orderBy(f.desc('salary')).rowsBetween(-2, 2)\n",
    "\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn(\n",
    "        'avg_salary',\n",
    "        # Apply the aggregation function to the limited window\n",
    "        f.avg('salary').over(window)\n",
    "    )\n",
    "    .withColumn(\n",
    "        'salary_diff',\n",
    "        f.col('salary') - f.col('avg_salary')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row function\n",
    "\n",
    "Besides aggregation functions, we can also apply the `row_number()` function over a window.\n",
    "\n",
    "This function assigns a unique sequential integer to each row within a partition, according to the order in which the rows appear in the partition. The first row gets the number 1, the second row gets the number 2, and so on.\n",
    "\n",
    "Let's rank the employees by salary in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a window ordered by 'salary'\n",
    "# Since this window is not partitioned, the whole dataset is used for sorting\n",
    "window = Window.orderBy(f.desc('salary'))\n",
    "\n",
    "# Apply the `row_number` window function to the dataset\n",
    "# over the defined window\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('row_number', f.row_number().over(window))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank function\n",
    "\n",
    "The `rank()` function is similar to the `row_number()` function, but it does not assign a unique sequential integer to each row. Instead, it assigns the same rank to rows with the same value.\n",
    "\n",
    "Let's rank the employees by salary in descending order using the `rank()` function and compare the results with the `row_number()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a window ordered by 'salary'\n",
    "# Since this window is not partitioned, the whole dataset is used for sorting\n",
    "window = Window.orderBy(f.desc('salary'))\n",
    "\n",
    "# Apply the `rank` window function to the dataset\n",
    "# over the defined window\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('row_number', f.rank().over(window))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference? Jessica and Ivy got the same rank because they have the same salary.\n",
    "\n",
    "Now let's apply the `rank()` to find the salary ranking by gender.\n",
    "\n",
    "This time, the rank will be calculated for each partition of the column `gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('gender').orderBy(f.desc('salary'))\n",
    "\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('salary_rank', f.rank().over(window))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Rank Function\n",
    "\n",
    "This function works similarly to the `rank()` function, but it does not leave gaps between consecutive ranks. If two rows have the same value, they will receive the same rank, and the next row will receive the next rank available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('gender').orderBy(f.desc('salary'))\n",
    "\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('salary_rank', f.dense_rank().over(window))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead and Lag Functions\n",
    "\n",
    "Lead and lag are functions that are applied to each dataset row, within a window partition.\n",
    "\n",
    "Lead returns the value of a column at a specified offset after the current row.\n",
    "Lag returns the value of a column at a specified offset before the current row.\n",
    "\n",
    "Let's look at the salary difference between employees within a department. We want to quantify the salary difference between each employee and the next employee earning more within the same department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window\n",
    "window = Window.partitionBy('department').orderBy('salary')\n",
    "\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('next_salary', f.lead(col=f.col('salary'), offset=1).over(window))\n",
    "    .withColumn(\n",
    "        'salary_diff',\n",
    "        f.col('next_salary') - f.col('salary')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also do the reverse operation and see how much more each employee earns compared to the next employee earning less within the same department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window\n",
    "window = Window.partitionBy('department').orderBy('salary')\n",
    "\n",
    "(\n",
    "    df_employees\n",
    "    .withColumn('prev_salary', f.lag(col=f.col('salary'), offset=1).over(window))\n",
    "    .withColumn(\n",
    "        'salary_diff',\n",
    "        f.col('salary') - f.col('prev_salary')\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that you know how to use the main window functions, go to the `exercises` notebook and practice what you've learned!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
