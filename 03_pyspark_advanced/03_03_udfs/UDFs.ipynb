{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, BooleanType, DoubleType, FloatType, DateType, StructType, StructField, ArrayType\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDFs\n",
    "\n",
    "In this notebook, you'll learn how to create and use User Defined Functions (UDFs) in PySpark. UDFs are a way to extend the functionality of PySpark by creating custom functions that can be used in PySpark SQL queries.\n",
    "\n",
    "These functions are especially useful when you need to perform complex operations on your data that are not supported by the built-in functions in PySpark.\n",
    "\n",
    "Let's start by creating the dummy orders DataFrame that we've used in the previous notebook. This DataFrame contains information about orders placed by customers, and has a column with complex data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    {\"order_id\": 5642, \"order_date\": datetime.strptime(\"2024-05-18\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"quantity\": 1.0, \"price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.7, \"price\": 1.99}],\n",
    "    'items_discount': ['Apple']},\n",
    "    {\"order_id\": 9762, \"order_date\": datetime.strptime(\"2024-05-02\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 0.5, \"price\": 6.99},\n",
    "        {\"name\": \"Apple\", \"quantity\": 3.0, \"price\": 2.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 2.5, \"price\": 3.39}],\n",
    "    'items_discount': ['Apple', 'Peach']},\n",
    "    {\"order_id\": 3652, \"order_date\": datetime.strptime(\"2024-05-23\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.5, \"price\": 1.99}],\n",
    "    'items_discount': []},\n",
    "    {\"order_id\": 1276, \"order_date\": datetime.strptime(\"2024-05-10\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Apple\", \"quantity\": 2.0, \"price\": 2.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 0.5, \"price\": 1.99},\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 1.0, \"price\": 3.39}],\n",
    "    'items_discount': ['Peach', 'Banana']},\n",
    "    {\"order_id\": 8763, \"order_date\": datetime.strptime(\"2024-05-14\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Strawberry\", \"quantity\": 1.0, \"price\": 6.99},\n",
    "        {\"name\": \"Peach\", \"quantity\": 1.0, \"price\": 3.39},\n",
    "        {\"name\": \"Mango\", \"quantity\": 1.5, \"price\": 5.99}],\n",
    "    'items_discount': ['Mango']},\n",
    "    {\"order_id\": 7652, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.0, \"price\": 1.99},\n",
    "        {\"name\": \"Mango\", \"quantity\": 1.5, \"price\": 5.99}],\n",
    "    'items_discount': ['Mango', 'Banana']},\n",
    "    {\"order_id\": 7631, \"order_date\": datetime.strptime(\"2024-05-22\", \"%Y-%m-%d\").date(),\n",
    "    \"items\": [\n",
    "        {\"name\": \"Banana\", \"quantity\": 1.0, \"price\": 1.99},\n",
    "        {\"name\": \"Banana\", \"quantity\": 2.5, \"price\": 1.99},],\n",
    "    'items_discount': []}\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType(), False),\n",
    "    StructField('order_date', DateType(), False),\n",
    "    StructField(\n",
    "        'items',\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField('name', StringType(), False),\n",
    "                StructField('quantity', FloatType(), False),\n",
    "                StructField('price', FloatType(), False)\n",
    "            ]),\n",
    "            False\n",
    "        ),\n",
    "        False\n",
    "    ),\n",
    "    StructField(\"items_discount\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_fruitshop = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df_fruitshop.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the schema of this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs VS Built-in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDFs can be used to extract information from complex data types, such as arrays and structs. Let's see how we can answer some questions from the last notebook using UDFs instead of the built-in functions in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the name of each product in the 'items' column.\n",
    "\n",
    "In the last notebook, we used the `transform` spark function. This applies a function to each element in an array and returns an array of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform function. This receives a column and a function to apply to the column.\n",
    "fn = f.transform(f.col('items'), lambda x: x['name'])\n",
    "\n",
    "df_fruitshop_item_names = (\n",
    "    df_fruitshop\n",
    "    # Apply the transformation to the dataframe and save the results to a new column\n",
    "    .withColumn('item_names', fn)\n",
    ")\n",
    "\n",
    "df_fruitshop_item_names.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can achieve the same result using a UDF.\n",
    "\n",
    "First we need to define a python function that receives a certain argument and returns the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_item_names(items):\n",
    "    return [item['name'] for item in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to register this function as a UDF using the `udf` function from the `pyspark.sql.functions` module.\n",
    "\n",
    "The `udf` function receives two arguments: the python function that we want to use as a UDF, and the return type of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_item_names_udf = f.udf(extract_item_names, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use this UDF in a PySpark SQL query to extract the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_item_names = (\n",
    "    df_fruitshop\n",
    "    .withColumn(\n",
    "        'item_names',\n",
    "        extract_item_names_udf(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_item_names.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDF is applied to the column `items` of each row in the DataFrame. Therefore, it is important that the function we defined is expecting the same data type as the column we are applying it to.\n",
    "\n",
    "Out function is expecting a list of structs, just like the `items` column in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the number of items in each order.\n",
    "\n",
    "In the last notebook, we used the `size` spark function. This returns the number of elements in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_size = (\n",
    "    df_fruitshop_item_names\n",
    "    .withColumn(\n",
    "        'nr_items',\n",
    "        f.size(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_size.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can achieve the same result using a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a python function that receives a list of items and returns the size of that list\n",
    "def count_items(items):\n",
    "    return len(items)\n",
    "\n",
    "# Register the function as a UDF and define the return type\n",
    "count_items_udf = f.udf(count_items, IntegerType())\n",
    "\n",
    "# Apply the UDF to the items column\n",
    "df_fruitshop_size = (\n",
    "    df_fruitshop_item_names\n",
    "    .withColumn(\n",
    "        'nr_items',\n",
    "        count_items_udf(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_size.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the number of unique item names in each order.\n",
    "\n",
    "In the last notebook, we used the `size` and `array_distinct` spark functions. The `array_distinct` function returns the distinct elements in an array, while the `size` function returns the number of elements in that array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_unique = (\n",
    "    df_fruitshop_size\n",
    "    .withColumn(\n",
    "        'unique_item_names',\n",
    "        f.array_distinct(f.col('item_names'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'nr_unique_items',\n",
    "        f.size(f.col('unique_item_names'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_unique.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using UDFs, we can combine these two functions into a single UDF.\n",
    "\n",
    "When our UDF is applied to the `items` column, we can get the number of unique item names in each order using python functions instead of the built-in functions in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a python function that receives a list of items and returns the number of unique items\n",
    "def count_unique_items(items):\n",
    "   # Get the unique items using the set data structure in python\n",
    "   unique_items_set = set([item['name'] for item in items])\n",
    "   # Return the length of the set\n",
    "   return len(unique_items_set)\n",
    "\n",
    "# Register the function as a UDF and define the return type\n",
    "count_unique_items_udf = f.udf(count_unique_items, IntegerType())\n",
    "\n",
    "df_fruitshop_unique = (\n",
    "    df_fruitshop_size\n",
    "    .withColumn(\n",
    "        'nr_unique_items',\n",
    "        count_unique_items_udf(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_unique.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check if an order contains a specific item (Banana)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we used the `array_contains` spark function. This function returns true if the specified value is found in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fruitshop_banana = (\n",
    "    df_fruitshop_unique\n",
    "    .withColumn(\n",
    "        'has_banana',\n",
    "        f.array_contains(f.col('item_names'), 'Banana')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_banana.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we do not need any intermediate column to check if 'Banana' is in the 'items' column. We can apply a single UDF to the 'items' column to check if 'Banana' is in the list of items for each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_banana(items):\n",
    "    return 'Banana' in [item['name'] for item in items]\n",
    "\n",
    "contains_banana_udf = f.udf(contains_banana, BooleanType())\n",
    "\n",
    "df_fruitshop_banana = (\n",
    "    df_fruitshop_unique\n",
    "    .withColumn(\n",
    "        'has_banana',\n",
    "        contains_banana_udf(f.col('items'))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_fruitshop_banana.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to create and use UDFs in PySpark.\n",
    "\n",
    "However, we have only recreated the functionality of already existing PySpark functions. UDFs are especially useful when you need to perform complex operations on your data that are not supported by the built-in functions in PySpark.\n",
    "\n",
    "Let's see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Calculate the average price per unit of each order, considering only items that were bought in a quantity greater than 1.\n",
    "\n",
    "Let's break the question into steps:\n",
    "- For each order, consider only items with quantity greater than 1.\n",
    "- Calculate the total price and the total quantity of items bought in each order.\n",
    "- Calculate the average price per unit of each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate average price per unit for items with quantity > 1\n",
    "def avg_price_per_unit(items):\n",
    "    total_price = 0.0\n",
    "    total_quantity = 0.0\n",
    "\n",
    "    # Iterate over the items and calculate the total price and quantity\n",
    "    for item in items:\n",
    "        if item['quantity'] > 1:\n",
    "            total_price += item['price'] * item['quantity']\n",
    "            total_quantity += item['quantity']\n",
    "\n",
    "    # Calculate the average price per unit\n",
    "    if total_quantity > 0:\n",
    "        return total_price / total_quantity\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "avg_price_per_unit_udf = f.udf(avg_price_per_unit, DoubleType())\n",
    "\n",
    "# Apply the UDF to calculate average price per unit for each item\n",
    "avg_price_per_unit_df = df_fruitshop.withColumn(\n",
    "    'avg_price_per_unit',\n",
    "    avg_price_per_unit_udf(f.col('items'))\n",
    ")\n",
    "\n",
    "avg_price_per_unit_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Identify the order with the highest total cost, considering both item price and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate total cost for each order\n",
    "def calculate_total_cost(items):\n",
    "    total_cost = 0\n",
    "\n",
    "    for item in items:\n",
    "        total_cost += item['quantity'] * item['price']\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "calculate_total_cost_udf = f.udf(calculate_total_cost, DoubleType())\n",
    "\n",
    "# Add a column with the total cost for each order\n",
    "df_fruitshop_with_total_cost = (\n",
    "    df_fruitshop\n",
    "    .withColumn(\n",
    "        'total_cost',\n",
    "        calculate_total_cost_udf(f.col('items'))\n",
    "    )\n",
    "    .orderBy(f.desc(f.col('total_cost')))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "df_fruitshop_with_total_cost.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDFs\n",
    "\n",
    "Pandas UDFs are similar to regular UDFs, but they operate on Pandas DataFrames instead of individual rows. This allows you to perform vectorized operations on Pandas DataFrame chunks, which are more efficient than row-by-row operations typical in regular UDFs.\n",
    "\n",
    "Here are two of the main advantages of using Pandas UDFs:\n",
    "\n",
    "1. **Performance Optimization:** Pandas UDFs can perform vectorized operations on Pandas DataFrame chunks, which are more efficient than row-by-row operations typical in regular UDFs.\n",
    "2. **Ease of Use and Familiarity:** Pandas UDFs allow you to leverage the rich functionality of Pandas, including its extensive libraries for data manipulation, statistical operations, and more.\n",
    "\n",
    "Let's download some data to demonstrate how to use Pandas UDFs in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh \n",
    "\n",
    "wget https://raw.githubusercontent.com/inesmcm26/lp-big-data/main/data/spotify.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "\n",
    "cp file:/databricks/driver/spotify.json dbfs:/FileStore/lp-big-data/spotify.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we've downloaded contains information about spotify playlists. Let's look at the schema and the first few rows of this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify = (\n",
    "    spark.read.format('json')\n",
    "    .load(\"/FileStore/lp-big-data/spotify.json\")\n",
    ")\n",
    "\n",
    "df_spotify.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spotify.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the schema, each record corresponds to a playlist.\n",
    "\n",
    "Each playlist is characterized by its name, the number of followers, a collaborative indicator, and a list of tracks. This list of tracks is a complex data type, as it contains a list of structs with information about each track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's answer some questions to demonstrate how you can use Pandas UDFs in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the number of tracks in each playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple question and we can answer it using built-in functions in PySpark.\n",
    "\n",
    "Let's do it and register the time it takes to run this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_num_tracks = df_spotify.withColumn(\"num_tracks\", f.size(f.col(\"tracks\")))\n",
    "\n",
    "df_num_tracks.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_spark_functions = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a simple UDF to answer this question. Let's see how we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a python function to counts the lenght of a list of tracks\n",
    "def count_tracks(tracks):\n",
    "    return len(tracks)\n",
    "\n",
    "# Register the function as a UDF and define the return type\n",
    "count_tracks_udf = f.udf(count_tracks, IntegerType())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Apply the UDF to the tracks column\n",
    "df_num_tracks = df_spotify.withColumn(\"num_tracks\", count_tracks_udf(f.col(\"tracks\")))\n",
    "\n",
    "df_num_tracks.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_udf = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how we can use a Pandas UDF to answer this question.\n",
    "\n",
    "A pandas UDF is a special type of udf that uses a Pandas DataFrame/Series as input and returns a Pandas DataFrame/Series as output. This allows you to perform vectorized operations on Pandas DataFrame chunks, which are more efficient than row-by-row operations typical in regular UDFs.\n",
    "\n",
    "To define a Pandas UDF, you need to use the `pandas_udf` decorator from the `pyspark.sql.functions` module. This decorator receives the return type of the function as argument.\n",
    "\n",
    "Since we will apply the Pandas UDF to the 'tracks' column, our function should expect a pandas Series as input. It will return the length of each element in the Series, which corresponds to another Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pandas UDF decorator\n",
    "@f.pandas_udf(IntegerType())\n",
    "def count_tracks(tracks: pd.Series) -> pd.Series:\n",
    "    # Use the 'apply' method of a pandas Series to count the number of tracks in each row\n",
    "    return tracks.apply(len)\n",
    "\n",
    "# Apply the Pandas UDF to the tracks column\n",
    "start = time.time()\n",
    "\n",
    "df_num_tracks = df_spotify.withColumn(\"num_tracks\", count_tracks(f.col(\"tracks\")))\n",
    "\n",
    "df_num_tracks.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_pandas_udf = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the performance of the three methods we used to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['Built-In Functions', 'UDF', 'Pandas UDF'], y=[time_spark_functions, time_udf, time_pandas_udf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using bulit-in functions is the fastest way to answer this question.\n",
    "\n",
    "Using Pandas UDFs is more efficient than using regular UDFs, but it is still slower than using built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the number of albums in each playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using pyspark built-in functions\n",
    "start = time.time()\n",
    "\n",
    "df_num_albums = (\n",
    "    df_num_tracks\n",
    "    .withColumn(\"albums\", f.transform(f.col(\"tracks\"), lambda x: x[\"album_name\"]))\n",
    "    .withColumn(\"num_albums\", f.size(f.array_distinct(f.col(\"albums\"))))\n",
    ")\n",
    "\n",
    "df_num_albums.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_spark_functions = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using UDF\n",
    "def count_albums(tracks):\n",
    "    return len(set([song[\"album_name\"] for song in tracks]))\n",
    "\n",
    "count_albums_udf = f.udf(count_albums, IntegerType())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_num_albums = (\n",
    "    df_num_tracks\n",
    "    .withColumn(\"num_albums\", count_albums_udf(f.col(\"tracks\")))\n",
    ")\n",
    "\n",
    "df_num_albums.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_udf = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using pandas udfs\n",
    "@f.pandas_udf(IntegerType())\n",
    "def count_albums(tracks: pd.Series) -> pd.Series:\n",
    "    return tracks.apply(lambda x: len(set([song[\"album_name\"] for song in x])))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_num_albums = (\n",
    "    df_num_tracks\n",
    "    .withColumn(\"num_albums\", count_albums(f.col(\"tracks\")))\n",
    ")\n",
    "\n",
    "df_num_albums.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_pandas_udf = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of the three methods we used to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['Built-In Functions', 'UDF', 'Pandas UDF'], y=[time_spark_functions, time_udf, time_pandas_udf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the built-in function is slower, probably because we need to use two functions to get the desired result.\n",
    "\n",
    "On the other hand, Pandas UDFs are still more efficient than regular UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the total duration of each playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using pyspark built-in functions\n",
    "start = time.time()\n",
    "\n",
    "df_total_duration = (\n",
    "    df_num_albums\n",
    "    .withColumn('track', f.explode(f.col('tracks')))\n",
    "    .groupBy('playlist_name')\n",
    "    .agg(f.sum(f.col('track.duration_ms')).alias('total_duration'))\n",
    ")\n",
    "\n",
    "df_total_duration.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_spark_functions = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using UDF\n",
    "def sum_durations(tracks):\n",
    "    return sum([song['duration_ms'] for song in tracks])\n",
    "\n",
    "sum_durations_udf = f.udf(sum_durations, IntegerType())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_total_duration = (\n",
    "    df_num_albums\n",
    "    .withColumn('total_duration', sum_durations_udf(f.col('tracks')))\n",
    ")\n",
    "\n",
    "df_total_duration.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_udf = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using pandas udfs\n",
    "@f.pandas_udf(IntegerType())\n",
    "def sum_durations(tracks: pd.Series) -> pd.Series:\n",
    "    return tracks.apply(lambda x: sum([song['duration_ms'] for song in x]))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_total_duration = (\n",
    "    df_num_albums\n",
    "    .withColumn('total_duration', sum_durations(f.col('tracks')))\n",
    ")\n",
    "\n",
    "df_total_duration.display()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_pandas_udf = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's compare the performance of the three methods we used to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['Built-In Functions', 'UDF', 'Pandas UDF'], y=[time_spark_functions, time_udf, time_pandas_udf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that using built-in functions is faster than using regular UDFs, but slower than using Pandas UDFs.\n",
    "\n",
    "This is because the operation we are computing is complex and leveraging the rich functionality of Pandas is more efficient than using built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finally, let's predict the number of followers of each playlist based on the number of tracks, number of albums, and total duration.\n",
    "\n",
    "To answer this question we should use a Pandas UDF. Firstly, because it is a very complex question that involves multiple operations.\n",
    "\n",
    "Secondly, because we can perform vectorized operations on Pandas DataFrame chunks, which are more efficient than row-by-row operations typical in regular UDFs.\n",
    "\n",
    "Let's use a linear regression model to predict the number of followers of each playlist based on the number of tracks, number of albums, and total duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(FloatType())\n",
    "def predict_num_followers_udf(\n",
    "    num_tracks: pd.Series,\n",
    "    num_albums: pd.Series,\n",
    "    total_duration: pd.Series,\n",
    "    num_followers: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"Predict the number of followers of a spotify playlist based\n",
    "    on the number of tracks, albums and total duration\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.vstack([num_tracks, num_albums, total_duration]).T\n",
    "    y = num_followers.values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "df_predictions = (\n",
    "    df_total_duration\n",
    "    .withColumn('predicted_num_followers',\n",
    "                predict_num_followers_udf(\n",
    "                    f.col('num_tracks'),\n",
    "                    f.col('num_albums'),\n",
    "                    f.col('total_duration'),\n",
    "                    f.col('num_followers'))\n",
    "                )\n",
    ")\n",
    "\n",
    "df_predictions.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to create and use UDFs and Pandas UDFs in PySpark. Go to the `exercises` notebook and practice what you've learned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daredata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
