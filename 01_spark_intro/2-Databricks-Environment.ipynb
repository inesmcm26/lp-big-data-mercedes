{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "In this notebook, you will learn about the Databricks platform and how to interact with it, the Databricks File System (DBFS), and how to run SQL queries in Databricks.\n",
    "\n",
    "1. [Getting started with Databricks](#Getting-started-with-Databricks)\n",
    "    1. [Account Setup](#Account-Setup)\n",
    "    2. [Importing notebooks to Databricks](#Importing-notebooks-to-Databricks)\n",
    "    3. [Interacting with Spark](#Interacting-with-Spark)\n",
    "    3. [Databricks File System (DBFS)](#Databricks-File-System-(DBFS))\n",
    "    4. [SQL in Databricks](#SQL-in-Databricks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Databricks\n",
    "\n",
    "[Databricks](https://www.databricks.com/) was founded by the creators of Apache Spark, using Spark's distributed computing framework at its core. This setup ensures efficient analytics and scalable data processing.\n",
    "\n",
    "It was originally designed as a user-friendly interface for Spark and simplifies complex distributed computing tasks. One notable feature of Databricks is its ability to **simplify Spark cluster management**, allowing data professionals to focus on data analysis rather than infrastructure.\n",
    "\n",
    "Resources can be easily adjusted based on workload demands, optimizing performance and cost-effectiveness. Users can specify cluster sizes, while Databricks handles the technical details.\n",
    "\n",
    "Databricks has since evolved into a comprehensive analytics solution, offering key features such as:\n",
    "\n",
    "- **Data Integration:** Seamlessly integrates with popular data storage systems like AWS S3, Azure Data Lake Storage, and Google Cloud Storage, as well as third-party tools, ensuring smooth data integration and interoperability. It abstracts the complexities of data storage and access, allowing users to focus on data analysis.\n",
    "\n",
    "- **Collaboration and Sharing:** Provides a collaborative environment for teams to work together on shared notebooks, visualizations, and dashboards, promoting knowledge sharing and streamlined collaboration.\n",
    "\n",
    "- **Extended Capabilities:** Offers advanced features such as Delta Lake for reliable data lake management, MLflow for managing the machine learning lifecycle, support for Lakehouse architecture, combining the best of data lakes and data warehouses, and more.\n",
    "\n",
    "- **Unified Platform:** Unifies data engineering, data science, and business intelligence workflows within a single platform, enabling seamless transitions between data preparation, model development, and visualization. It supports workflows and jobs to automate tasks and streamline processes.\n",
    "\n",
    "- **Unity Catalog:** Recently introduced by Databricks, Unity Catalog is a unified metadata service that provides a comprehensive view of data assets across the organization, enabling data discovery, governance, and lineage tracking.\n",
    "\n",
    "In this course, we will be using the [Databricks Community Edition](https://community.cloud.databricks.com/?o=1730469639678502), which is a free version of Databricks that provides a limited but sufficient environment for learning and experimenting with Spark and Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account Setup\n",
    "\n",
    "Please, follow the instructions below to create an account and access the Databricks Community Edition.\n",
    "\n",
    "1. Click on this [link](https://community.cloud.databricks.com/login.html)\n",
    "2. Click on \"Sign up\".\n",
    "3. Add your information and click 'Continue'.\n",
    "4. Select the 'Get started with Community Edition' option.\n",
    "5. Check your email for a verification link.\n",
    "\n",
    "Once you have completed these steps, you will have access to the Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing notebooks to Databricks\n",
    "\n",
    "The Databricks Community Edition does not allow you to directly connect to a github repository. However, you can manually import notebooks to Databricks using an url from a github repository.\n",
    "\n",
    "To import this and the following notebooks to Databricks, follow these steps:\n",
    "1. Login to your Databricks account.\n",
    "2. Click in the 'Workspace' tab.\n",
    "3. Select 'Users' and then your username.\n",
    "4. Create a new folder or select an existing one.\n",
    "5. With the folder selected, right-click and select 'Import'.\n",
    "6. Select 'URL' and paste the URL of the notebook you want to import from github.\n",
    "\n",
    "Now you can run this notebook in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Spark\n",
    "\n",
    "Databricks provides a user-friendly interface for interacting with Spark, allowing users to run Spark code in a notebook environment. Let's see how we can interact with Spark in Databricks.\n",
    "\n",
    "First, make sure you have a running cluster attached to your notebook. If you don't have a cluster running, you can start one by clicking on the `Connect` button on the topbar of the Databricks interface, then clicking the `Create new resource` button.\n",
    "\n",
    "***Note:*** When you are using the Databricks Community Edition, you may need to start a new cluster each time you want to run code, as the cluster will be terminated after a period of inactivity. Unfortunately, you will not be able to re-initialize the same cluster after it has been terminated, so you will need to create a new cluster each time you want to run code.\n",
    "\n",
    "Databricks notebooks provide, by default, a **SparkSession** object called `spark`, which is already initialized and ready to use. You can access this object by simply calling `spark`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html) you can find all SparkSession methods.\n",
    "\n",
    "Similarly, there is also the object `sc` that allows you to interact with the **SparkContext**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how to use these object in detail on the next sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks File System (DBFS)\n",
    "\n",
    "The **Databricks File System ([DBFS](https://docs.databricks.com/en/dbfs/index.html))** is as a distributed file system that allows users to store and access data within the Databricks environment.\n",
    "\n",
    "It operates as a cloud-supported file system, acting as an abstraction layer over various data storage systems like AWS S3, Azure Data Lake Storage, and Google Cloud Storage.\n",
    "\n",
    "This abstraction simplifies data interactions within Databricks, enabling users to easily access and manipulate data without worrying about underlying storage details.\n",
    "\n",
    "It enables users to interact with data through familiar file system operations, such as reading, writing, and deleting files It also supports several file formats, including Parquet, CSV, JSON, and Delta Lake.\n",
    "\n",
    "DBFS-stored data can be accessed across various Databricks components, such as notebooks, jobs, and clusters, simplifying data sharing and collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to interact with the DBFS via command line using shell commands.\n",
    "\n",
    "Since Databricks notebook cells can run Python, Scala, SQL, and shell commands, we can use shell commands to interact with the DBFS in notebooks.\n",
    "\n",
    "To execute shell in a databricks notebook cell you can write `%sh` in the beginning of the cell and then write the shell command.\n",
    "\n",
    "Let's run our first shell command!\n",
    "\n",
    "Check your working directory using the `pwd` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the current directory path is `/databricks/driver`. This is the default directory where Databricks stores files in the local file system of the driver node.\n",
    "\n",
    "Now, check inside the working directory using the `ls` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some data into using the `wget` shell command. This command receives a downloadable link and downloads the file into the current directory.\n",
    "\n",
    "The file we are going to download is a text file containing an excerpt from the 1984 book by George Orwell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "wget \"https://github.com/inesmcm26/lp-big-data/raw/main/data/1984.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the working directory contents again to guarantee the file has been downloaded and take a look at the file using the `cat` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sh\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "cat 1984.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Databricks notebooks, when we execute standard bash shell commands, they are accessing the **driver local storage**.\n",
    "\n",
    "This storage space is temporary and resides on the driver node of your Spark cluster. It is useful for storing files needed for your Spark jobs, like configuration files, libraries, or intermediate results. However, this storage is not permanent, and it gets deleted once your cluster restarts or is terminated.\n",
    "\n",
    "In contrast, there's the DBFS that provides a distributed file system that allows you to store and access data across your Spark cluster. It can store data that needs to be accessed by multiple nodes in your cluster and persists even after your cluster is terminated.\n",
    "\n",
    "| Feature | Driver Local Storage | DBFS |\n",
    "|---------|--------------------------|------|\n",
    "| Location | Local disk storage | Distributed file system |\n",
    "| Persistence | Not persistent, deleted when cluster is terminated or restarted | Persistent, not deleted when cluster is terminated or restarted |\n",
    "| Access | /databricks/driver | dbfs:/ scheme with dbutils.fs module |\n",
    "| Speed | Faster | Slower |\n",
    "| Cost | Cheaper | More expensive |\n",
    "| Reliability | Not reliable | Reliable |\n",
    "| Scalability | Not scalable | Scalable |\n",
    "| Use case | Temporary data needed for a specific Spark job | Data needed across multiple Spark jobs or that cannot be regenerated if lost |\n",
    "\n",
    "\n",
    "The DBFS serves as the default location for Spark commands. Therefore, commands like `spark.read` won't work unless the files it is trying to read are stored in the DBFS or a very specific path to the driver local storage is provided.\n",
    "\n",
    "Check it out by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"/databricks/driver/1984.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to access the DBFS is to use write `%fs` instead of `%sh` in the beginning of the notebook cell.\n",
    "\n",
    "This command line interface allows you to run similar commands to the bash shell ones but with the DBFS as the working directory.\n",
    "\n",
    "Let's check the contents of the DBFS root directory using the `%fs ls` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can copy the 1984 excerpt to the DBFS and read it using spark again.\n",
    "\n",
    "Copy the data to the `FileStore` folder as it is a DBFS'S folder that has optimized access from within notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "\n",
    "cp file:/databricks/driver/1984.txt /FileStore/1984.txt\n",
    "\n",
    "ls FileStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text('/FileStore/1984.txt').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really wanted to access the file without moving it from the driver local storage, you could specify the path to the file in the driver local storage beginning with `file:/` like in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.text(\"file:/databricks/driver/1984.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of interacting with the DBFS is through the Databricks utilities `dbutils.fs` object. This object provides a set of file system operations to interact with the DBFS. These operations can be equally run in a Databricks notebook cell using the `%fs` magic command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(path=\"/FileStore/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL in DataBricks\n",
    "\n",
    "In Databricks, running SQL becomes almost native to the platform. You can run SQL queries in a Databricks notebook cell by using the `%sql` magic command.\n",
    "\n",
    "Let's look at an example. But first, let's download some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "wget https://raw.githubusercontent.com/inesmcm26/lp-big-data/main/data/employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "\n",
    "cp file:/databricks/driver/employees.csv /FileStore/employees.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query data in a notebook cell, you first need to create a temporary view or a table.\n",
    "\n",
    "When you create a temporary view or table from a CSV file (or any other data source), you are essentially **creating a metadata reference** that allows SQL queries to operate on the underlying data stored in memory or on disk.\n",
    "\n",
    "Behind the scenes, the SQL query is translated into Spark SQL's logical execution plan. This plan defines the series of transformations and actions that Spark needs to perform to fulfill the query. Spark optimizes the execution of SQL queries, leveraging its distributed computing capabilities to process data efficiently across a cluster of machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW df_employees\n",
    "USING CSV\n",
    "OPTIONS (path \"/FileStore/employees.csv\", header \"true\", delimiter \",\", mode \"FAILFAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT *\n",
    "FROM df_employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create temporary views from a Spark DataFrame.\n",
    "\n",
    "We'll dive into spark DataFrame later! For now, let's just see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([{\"ID\": 1, \"Name\": \"John\"}, {\"ID\": 2, \"Name\": \"Isabel\"}, {\"ID\": 3, \"Name\": \"Bart\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run SQL queries directly on the data that is saved on a  DataFrame, you will need to register it to a temporary view.\n",
    "\n",
    "For that, you can use the `createOrReplaceTempView` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('spark_df_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT *\n",
    "FROM spark_df_sample\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just as it is usual in SQL databases, you can list all the tables available in the Spark catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the views created in the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SHOW VIEWS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can also list the available databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Databricks, when you run this command, you are actually **interacting with a metastore database associated with your Databricks workspace**. This metastore database serves as a catalog for storing metadata about databases, tables, views, and other objects within your Databricks environment.\n",
    "\n",
    "So, when you run the `SHOW DATABASES;` command, you're querying the metastore database to retrieve a list of databases defined within your Databricks workspace. Each database can contain tables and views that you create or import into Databricks for data analysis and processing.\n",
    "\n",
    "The metastore database is initialized in the running cluster and can not be accessed from outside the cluster. When the cluster is terminated, the metastore database is also terminated and you can no longer access the tables and views you created in the same way.\n",
    "\n",
    "However, the tables do not disappear. They are still stored in the DBFS and can be accessed by creating a new cluster and re-reading the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
